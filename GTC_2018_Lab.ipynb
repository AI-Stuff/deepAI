{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## \"Detection of Anomalies in Financial Transactions using Deep Autoencoder Networks\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This GPU Technology Conference (GTC) 2018 lab was developed by Mr. X, and Mr. Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Environment Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 01.1 Python Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, let's verify that Python is working on your system. To do this, execute the cell block below by giving it focus (clicking on it with your mouse), and hitting Shift-Enter, or pressing the play button in the toolbar above. If all goes well, you should see some output returned below the grey cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The answer should be forty-two: {}'.format(str(40+2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 01.2 Import Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing utilities\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# importing pytorch libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import autograd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# importing data science libraries\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 01.3 CUDNN / GPU Verficiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print CUDNN backend version\n",
    "print('The CUDNN backend version: {}'.format(torch.backends.cudnn.version()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's execute the cell below to display information about the GPUs running on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's execute the cell below to display information about the PyTorch version running on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print current PyTorch version\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] PyTorch version: {}'.format(now, torch.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Lab Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo -- Timur and Marco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" style=\"max-width: 550px; height: auto\" src=\"images/accounting.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" style=\"max-width: 550px; height: auto\" src=\"images/anomalies.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" style=\"max-width: 400px; height: auto\" src=\"images/cube.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Autoencoder Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03.1 Introduction to Autoencoder Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" style=\"max-width: 600px; height: auto\" src=\"images/autoencoder.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03.2 Implementing the Encoder Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(encoder, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.0, inplace=True)\n",
    "\n",
    "        self.encoder_L1 = nn.Linear(186, 512, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L1.weight)\n",
    "        self.encoder_R1 = nn.LeakyReLU(negative_slope= 0.4, inplace=True)\n",
    "\n",
    "        self.encoder_L2 = nn.Linear(512, 256, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L2.weight)\n",
    "        self.encoder_R2 = nn.LeakyReLU(negative_slope= 0.4, inplace=True)\n",
    "\n",
    "        self.encoder_L3 = nn.Linear(256, 128, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L3.weight)\n",
    "        self.encoder_R3 = nn.LeakyReLU(negative_slope= 0.4, inplace=True)\n",
    "\n",
    "        self.encoder_L4 = nn.Linear(128, 64, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L4.weight)\n",
    "        self.encoder_R4 = nn.LeakyReLU(negative_slope= 0.4, inplace=True)\n",
    "\n",
    "        self.encoder_L5 = nn.Linear(64, 32, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L5.weight)\n",
    "        self.encoder_R5 = nn.LeakyReLU(negative_slope= 0.4, inplace=True)\n",
    "\n",
    "        self.encoder_L6 = nn.Linear(32, 16, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L6.weight)\n",
    "        self.encoder_R6 = nn.LeakyReLU(negative_slope= 0.4, inplace=True)\n",
    "\n",
    "        self.encoder_L7 = nn.Linear(16, 8, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L7.weight)\n",
    "        self.encoder_R7 = nn.LeakyReLU(negative_slope= 0.4, inplace=True)\n",
    "\n",
    "        self.encoder_L8 = nn.Linear(8, 4, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L8.weight)\n",
    "        self.encoder_R8 = nn.LeakyReLU(negative_slope= 0.4, inplace=True)\n",
    "\n",
    "        self.encoder_L9 = nn.Linear(4, 3, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L9.weight)\n",
    "        self.encoder_R9 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.encoder_R1(self.dropout(self.encoder_L1(x)))\n",
    "        x = self.encoder_R2(self.dropout(self.encoder_L2(x)))\n",
    "        x = self.encoder_R3(self.dropout(self.encoder_L3(x)))\n",
    "        x = self.encoder_R4(self.dropout(self.encoder_L4(x)))\n",
    "        x = self.encoder_R5(self.dropout(self.encoder_L5(x)))\n",
    "        x = self.encoder_R6(self.dropout(self.encoder_L6(x)))\n",
    "        x = self.encoder_R7(self.dropout(self.encoder_L7(x)))\n",
    "        x = self.encoder_R8(self.dropout(self.encoder_L8(x)))\n",
    "        x = self.encoder_R9(self.encoder_L9(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03.3 Implementing the Decoder Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(decoder, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.0, inplace=True)\n",
    "\n",
    "        self.decoder_L1 = nn.Linear(3, 4, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L1.weight)\n",
    "        self.decoder_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.decoder_L2 = nn.Linear(4, 8, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L2.weight)\n",
    "        self.decoder_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.decoder_L3 = nn.Linear(8, 16, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L3.weight)\n",
    "        self.decoder_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.decoder_L4 = nn.Linear(16, 32, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L4.weight)\n",
    "        self.decoder_R4 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.decoder_L5 = nn.Linear(32, 64, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L5.weight)\n",
    "        self.decoder_R5 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.decoder_L6 = nn.Linear(64, 128, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L6.weight)\n",
    "        self.decoder_R6 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.decoder_L7 = nn.Linear(128, 256, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L7.weight)\n",
    "        self.decoder_R7 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.decoder_L8 = nn.Linear(256, 512, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L8.weight)\n",
    "        self.decoder_R8 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.decoder_L9 = nn.Linear(512, 186, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L9.weight)\n",
    "        self.decoder_R9 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.decoder_R1(self.dropout(self.decoder_L1(x)))\n",
    "        x = self.decoder_R2(self.dropout(self.decoder_L2(x)))\n",
    "        x = self.decoder_R3(self.dropout(self.decoder_L3(x)))\n",
    "        x = self.decoder_R4(self.dropout(self.decoder_L4(x)))\n",
    "        x = self.decoder_R5(self.dropout(self.decoder_L5(x)))\n",
    "        x = self.decoder_R6(self.dropout(self.decoder_L6(x)))\n",
    "        x = self.decoder_R7(self.dropout(self.decoder_L7(x)))\n",
    "        x = self.decoder_R8(self.dropout(self.decoder_L8(x)))\n",
    "        x = self.decoder_R9(self.decoder_L9(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init training network classes / architectures\n",
    "encoder_train = encoder()\n",
    "decoder_train = decoder()\n",
    "\n",
    "# push to cuda if cudnn is available\n",
    "if (torch.backends.cudnn.version() != None):\n",
    "    encoder_train = encoder().cuda()\n",
    "    decoder_train = decoder().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the initialized architectures\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] encoder architecture:\\n\\n{}\\n'.format(now, encoder_train))\n",
    "print('[LOG {}] decoder architecture:\\n\\n{}\\n'.format(now, decoder_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Financial Fraud Detection Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo -- Timur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is extracted from Kaggle datasets repository at the following link: \n",
    "https://www.kaggle.com/ntnu-testimon/paysim1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "ori_dataset = pd.read_csv(\"./data/paysim1.zip\", sep=\",\", header=0, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension of data\n",
    "ori_dataset.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot \"step\" and \"type\" attributes\n",
    "sns.set()\n",
    "fig, ax =plt.subplots(1,2)\n",
    "fig.set_figwidth(15)\n",
    "sns.distplot(ori_dataset['step'], kde=False, bins=100, ax=ax[0])\n",
    "sns.countplot(x=ori_dataset['type'], ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have a look what \"type\" do fraudulent transactions possess\n",
    "ori_dataset[ori_dataset['isFraud'] == 1]['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we select a subset of \"type\": \"transfer\" (\"cash-out\" may be selected for optional excercise)\n",
    "ori_subset = ori_dataset[ori_dataset['type'] == 'TRANSFER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension of data\n",
    "ori_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot \"amount\" attribute and its log scale\n",
    "fig, ax =plt.subplots(1,2)\n",
    "fig.set_figwidth(15)\n",
    "sns.distplot(ori_subset['amount'], ax=ax[0])\n",
    "sns.distplot(ori_subset['amount'].apply(np.log), ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess numeric attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select numeric attributes\n",
    "numeric_attr_names = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "epsilon = 1e-7\n",
    "\n",
    "# add a small epsilon to eliminate zero values from data for log scaling\n",
    "numeric_attr = ori_subset[numeric_attr_names] + epsilon\n",
    "numeric_attr = numeric_attr.apply(np.log)\n",
    "\n",
    "# normalize all numeric attribute to the range [0,1]\n",
    "numeric_attr = (numeric_attr - numeric_attr.min()) / (numeric_attr.max() - numeric_attr.min())\n",
    "\n",
    "# append 'isFraud' attribute\n",
    "numeric_attr['isFraud'] = ori_subset['isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot numeric attributes scaled under natural log\n",
    "if os.path.exists('./images/numeric_attr_pair_plot.png'):\n",
    "    display(Image('./images/numeric_attr_pair_plot.png'))\n",
    "else:\n",
    "    sns.pairplot(data=numeric_attr, vars=numeric_attr_names, hue='isFraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of distinct values possessed by 'nameOrig'\n",
    "ori_subset['nameOrig'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we select first 2 or 3 characters\n",
    "nameOrig_slice_2 = ori_subset['nameOrig'].str.slice(0, 2)\n",
    "nameOrig_slice_3 = ori_subset['nameOrig'].str.slice(0, 3)\n",
    "\n",
    "nameDest_slice_2 = ori_subset['nameDest'].str.slice(0, 2)\n",
    "nameDest_slice_3 = ori_subset['nameDest'].str.slice(0, 3)\n",
    "\n",
    "fig, ax =plt.subplots(1,2)\n",
    "fig.set_figwidth(20)\n",
    "sns.countplot(x=nameOrig_slice_2, ax=ax[0])\n",
    "sns.countplot(x=nameOrig_slice_3, ax=ax[1])\n",
    "# g.set_xticklabels(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select categorical attributes\n",
    "ori_subset_categ = pd.concat([ori_subset['type'], nameOrig_slice_3, nameDest_slice_3], \n",
    "                             axis = 1,\n",
    "                             names = ['type', 'nameOrig', 'nameDest'])\n",
    "# select numeric attributes\n",
    "ori_subset_numeric = numeric_attr.drop('isFraud', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform categorcal attributes into a sparse representation uning one-hot transformation\n",
    "ori_subset_categ_transformed = pd.get_dummies(ori_subset_categ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join categorical and numeric subsets\n",
    "ori_subset_transformed = pd.concat([ori_subset_categ_transformed, ori_subset_numeric],\n",
    "                                   axis = 1)\n",
    "ori_subset_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepate the dataset for pytorch loader\n",
    "torch_dataset = torch.from_numpy(ori_subset_transformed.values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = [\"AccountID_Key\", \"CurrencyCode_Key\", \"TaxCode_Key\", \"CompanyKey_Key\", \"ShipToCountry_Key\", \"ShipFromCountry_Key\"]\n",
    "# ranges = [0, 62+1, 121+1, 183+1, 234+1, 349+1, 400+1]\n",
    "\n",
    "# init training results\n",
    "# columns = [\"timestamp\", \"node\", \"seed\", \"architecture\", \"epoch\", \"rec_loss\", \"roc_auc\", \"anomalies\", \"normalies\", \"anomalies_s\", \"normalies_s\", \"max_threshold\", \"max_tpr_s\", \"max_fpr_s\", \"precision\", \"recall\", \"f1_score\", \"fpr\", \"tpr\", \"thresholds\"]\n",
    "# evaluations = [\"err_\" + str(element) for element in range(0, (len(features))+1)]\n",
    "# columns.extend(evaluations)\n",
    "# evaluations = [\"ano_c1_\" + str(element) for element in range(0, (len(features))+1)]\n",
    "# columns.extend(evaluations)\n",
    "# evaluations = [\"ano_c2_\" + str(element) for element in range(0, (len(features))+1)]\n",
    "# columns.extend(evaluations)\n",
    "# evaluation_results = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05. Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo - Timur and Marco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init deterministic seed\n",
    "seed_value = 1234 #4444 #3333 #2222 #1111 #1234\n",
    "rd.seed(seed_value) # set random seed\n",
    "np.random.seed(seed_value) # set numpy seed\n",
    "torch.manual_seed(seed_value) # set pytorch seed CPU\n",
    "# torch.cuda.manual_seed(seed_value) # set pytorch seed GPU\n",
    "\n",
    "# init training parameters\n",
    "num_epochs = 50\n",
    "mini_batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# convert to pytorch tensor - none cuda enabled\n",
    "dataloader = DataLoader(torch_dataset, batch_size=mini_batch_size, shuffle=True, num_workers=0)\n",
    "# set num_workers to zero to retreive deterministic results\n",
    "\n",
    "# determine if CUDA is available at compute node\n",
    "if (torch.backends.cudnn.version() != None):\n",
    "\n",
    "    dataloader = DataLoader(torch_dataset.cuda(), batch_size=mini_batch_size, shuffle=True)\n",
    "\n",
    "# define optimization criterion and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "encoder_optimizer = torch.optim.Adam(encoder_train.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.Adam(decoder_train.parameters(), lr=learning_rate)\n",
    "\n",
    "# train autoencoder model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # init mini batch counter\n",
    "    mini_batch_count = 0\n",
    "\n",
    "    # determine if CUDA is available at compute node\n",
    "    if (torch.backends.cudnn.version() != None) and (use_cuda == True):\n",
    "\n",
    "        # set all networks / models in CPU mode\n",
    "        encoder_train.cuda()\n",
    "        decoder_train.cuda()\n",
    "\n",
    "    # set networks in training mode (apply dropout when needed)\n",
    "    encoder_train.train()\n",
    "    decoder_train.train()\n",
    "\n",
    "    for mini_batch_data in dataloader:\n",
    "\n",
    "        # increase mini batch counter\n",
    "        mini_batch_count += 1\n",
    "\n",
    "        # convert mini batch to torch variable\n",
    "        mini_batch_torch = autograd.Variable(mini_batch_data)\n",
    "\n",
    "        # =================== forward pass =====================\n",
    "\n",
    "        # run forward pass\n",
    "        z_representation = encoder_train(mini_batch_torch) # encode mini-batch data\n",
    "        mini_batch_reconstruction = decoder_train(z_representation) # decode mini-batch data\n",
    "\n",
    "        # determine reconstruction loss\n",
    "        reconstruction_loss = criterion(mini_batch_reconstruction, mini_batch_torch)\n",
    "\n",
    "        # =================== backward pass ====================\n",
    "\n",
    "        # reset graph gradients\n",
    "        decoder_optimizer.zero_grad()\n",
    "        encoder_optimizer.zero_grad()\n",
    "\n",
    "        # run backward pass\n",
    "        reconstruction_loss.backward()\n",
    "\n",
    "        # update network parameters\n",
    "        decoder_optimizer.step()\n",
    "        encoder_optimizer.step()\n",
    "\n",
    "        # =================== log ==============================\n",
    "\n",
    "        if mini_batch_count % 100 == 0:\n",
    "\n",
    "            # print mini batch reconstuction results\n",
    "            now = datetime.utcnow().strftime(\"%Y%m%d-%H_%M_%S\")\n",
    "            print('[LOG {}] training status, epoch: [{:04}/{:04}], batch: {:04}, loss: {:.10f}'.format(now, (epoch+1), num_epochs, mini_batch_count, reconstruction_loss.data[0]))\n",
    "\n",
    "    # save trained encoder model file to disk\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d-%H_%M_%S\")\n",
    "    encoder_model_name = \"{}_ep_{}_encoder_model.pth\".format(now, (epoch+1))\n",
    "    torch.save(encoder_train.state_dict(), os.path.join(\"./models\", encoder_model_name))\n",
    "\n",
    "    # save trained decoder model file to disk\n",
    "    decoder_model_name = \"{}_ep_{}_decoder_model.pth\".format(now, (epoch+1))\n",
    "    torch.save(decoder_train.state_dict(), os.path.join(\"./models\", decoder_model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06. Result Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify trained models to be loaded\n",
    "encoder_model_name = \"20180211-09_02_16_ep_50_encoder_model.pth\"\n",
    "decoder_model_name = \"20180211-09_02_16_ep_50_decoder_model.pth\"\n",
    "\n",
    "# init training network classes / architectures\n",
    "encoder_eval = encoder()\n",
    "decoder_eval = decoder()\n",
    "\n",
    "# load trained models\n",
    "encoder_eval.load_state_dict(torch.load(os.path.join(\"models\", encoder_model_name)))\n",
    "decoder_eval.load_state_dict(torch.load(os.path.join(\"models\", decoder_model_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert encoded transactional data to torch Variable\n",
    "data = autograd.Variable(torch_dataset)\n",
    "\n",
    "# set networks in training mode (don't apply dropout)\n",
    "encoder_eval.eval()\n",
    "decoder_eval.eval()\n",
    "\n",
    "# reconstruct encoded transactional data\n",
    "reconstruction = decoder_eval(encoder_eval(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine reconstruction loss - all transactions\n",
    "reconstruction_loss_all = criterion(reconstruction, data)\n",
    "\n",
    "# print reconstruction loss - all transactions\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] collected reconstruction loss of: {:06}/{:06} transactions'.format(now, reconstruction.size()[0], reconstruction.size()[0]))\n",
    "print('[LOG {}] reconstruction loss: {:.10f}'.format(now, reconstruction_loss_all.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init binary cross entropy errors\n",
    "reconstruction_loss_transaction = np.zeros(reconstruction.size()[0])\n",
    "\n",
    "# iterate over all detailed reconstructions\n",
    "for i in range(0, reconstruction.size()[0]):\n",
    "\n",
    "    # determine reconstruction loss - individual transactions\n",
    "    reconstruction_loss_transaction[i] = criterion(reconstruction[i], data[i]).data[0]\n",
    "\n",
    "    if(i % 100000 == 0):\n",
    "\n",
    "        ### print conversion summary\n",
    "        now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "        print('[LOG {}] collected individial reconstruction loss of: {:06}/{:06} transactions'.format(now, i, reconstruction.size()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plot_data = np.column_stack((np.arange(len(reconstruction_loss_transaction)), \n",
    "                             reconstruction_loss_transaction))\n",
    "\n",
    "regular_data = plot_data[ori_subset.isFraud == 0]\n",
    "fraud_data = plot_data[ori_subset.isFraud == 1]\n",
    "\n",
    "# plot reconstruction error scatter plot\n",
    "ax.scatter(regular_data[:, 0], regular_data[:, 1], c='C0', alpha=0.4, marker=\"o\") # plot regular transactions\n",
    "ax.scatter(fraud_data[:, 0], fraud_data[:, 1], c='C1', marker=\"^\") # plot fraudulent transactions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07. Optional Excercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo - Timur and Marco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08. Lab Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo - Timur and Marco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 09. Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo - Timur and Marco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
