{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## \"Detection of Anomalies in Financial Transactions using Deep Autoencoder Networks\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This GPU Technology Conference (GTC) 2018 lab was developed by Mr. X, and Mr. Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Environment Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 01.1 Python Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, let's verify that Python is working on your system. To do this, execute the cell block below by giving it focus (clicking on it with your mouse), and hitting Shift-Enter, or pressing the play button in the toolbar above. If all goes well, you should see some output returned below the grey cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer should be forty-two: 42\n"
     ]
    }
   ],
   "source": [
    "print('The answer should be forty-two: {}'.format(str(40+2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 01.2 Import Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing utilities\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# importing pytorch libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import autograd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# importing data science libraries\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 01.3 CUDNN / GPU Verficiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CUDNN backend version: None\n"
     ]
    }
   ],
   "source": [
    "# print CUDNN backend version\n",
    "print('The CUDNN backend version: {}'.format(torch.backends.cudnn.version()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's execute the cell below to display information about the GPUs running on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: nvidia-smi: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's execute the cell below to display information about the PyTorch version running on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20180125-08:49:19] PyTorch version: 0.3.0.post4\n"
     ]
    }
   ],
   "source": [
    "# print current PyTorch version\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] PyTorch version: {}'.format(now, torch.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Lab Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo -- Timur and Marco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" style=\"max-width: 550px; height: auto\" src=\"images/accounting.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Autoencoder Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03.1 Introduction to Autoencoder Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" style=\"max-width: 600px; height: auto\" src=\"images/autoencoder.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03.2 Implementing the Encoder Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(encoder, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.0, inplace=True)\n",
    "\n",
    "        self.encoder_L1 = nn.Linear(401, 512, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L1.weight)\n",
    "        self.encoder_R1 = nn.LeakyReLU(negative_slope= 0.4, inplace=True)\n",
    "\n",
    "        self.encoder_L2 = nn.Linear(512, 256, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L2.weight)\n",
    "        self.encoder_R2 = nn.LeakyReLU(negative_slope= 0.4, inplace=True)\n",
    "\n",
    "        self.encoder_L3 = nn.Linear(256, 128, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L3.weight)\n",
    "        self.encoder_R3 = nn.LeakyReLU(negative_slope= 0.4, inplace=True)\n",
    "\n",
    "        self.encoder_L4 = nn.Linear(128, 64, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L4.weight)\n",
    "        self.encoder_R4 = nn.LeakyReLU(negative_slope= 0.4, inplace=True)\n",
    "\n",
    "        self.encoder_L5 = nn.Linear(64, 32, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L5.weight)\n",
    "        self.encoder_R5 = nn.LeakyReLU(negative_slope= 0.4, inplace=True)\n",
    "\n",
    "        self.encoder_L6 = nn.Linear(32, 16, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L6.weight)\n",
    "        self.encoder_R6 = nn.LeakyReLU(negative_slope= 0.4, inplace=True)\n",
    "\n",
    "        self.encoder_L7 = nn.Linear(16, 8, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L7.weight)\n",
    "        self.encoder_R7 = nn.LeakyReLU(negative_slope= 0.4, inplace=True)\n",
    "\n",
    "        self.encoder_L8 = nn.Linear(8, 4, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L8.weight)\n",
    "        self.encoder_R8 = nn.LeakyReLU(negative_slope= 0.4, inplace=True)\n",
    "\n",
    "        self.encoder_L9 = nn.Linear(4, 3, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L9.weight)\n",
    "        self.encoder_R9 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.encoder_R1(self.dropout(self.encoder_L1(x)))\n",
    "        x = self.encoder_R2(self.dropout(self.encoder_L2(x)))\n",
    "        x = self.encoder_R3(self.dropout(self.encoder_L3(x)))\n",
    "        x = self.encoder_R4(self.dropout(self.encoder_L4(x)))\n",
    "        x = self.encoder_R5(self.dropout(self.encoder_L5(x)))\n",
    "        x = self.encoder_R6(self.dropout(self.encoder_L6(x)))\n",
    "        x = self.encoder_R7(self.dropout(self.encoder_L7(x)))\n",
    "        x = self.encoder_R8(self.dropout(self.encoder_L8(x)))\n",
    "        x = self.encoder_R9(self.encoder_L9(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03.3 Implementing the Decoder Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(decoder, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.0, inplace=True)\n",
    "\n",
    "        self.decoder_L1 = nn.Linear(3, 4, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L1.weight)\n",
    "        self.decoder_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.decoder_L2 = nn.Linear(4, 8, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L2.weight)\n",
    "        self.decoder_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.decoder_L3 = nn.Linear(8, 16, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L3.weight)\n",
    "        self.decoder_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.decoder_L4 = nn.Linear(16, 32, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L4.weight)\n",
    "        self.decoder_R4 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.decoder_L5 = nn.Linear(32, 64, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L5.weight)\n",
    "        self.decoder_R5 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.decoder_L6 = nn.Linear(64, 128, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L6.weight)\n",
    "        self.decoder_R6 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.decoder_L7 = nn.Linear(128, 256, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L7.weight)\n",
    "        self.decoder_R7 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.decoder_L8 = nn.Linear(256, 512, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L8.weight)\n",
    "        self.decoder_R8 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.decoder_L9 = nn.Linear(512, 401, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L9.weight)\n",
    "        self.decoder_R9 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.decoder_R1(self.dropout(self.decoder_L1(x)))\n",
    "        x = self.decoder_R2(self.dropout(self.decoder_L2(x)))\n",
    "        x = self.decoder_R3(self.dropout(self.decoder_L3(x)))\n",
    "        x = self.decoder_R4(self.dropout(self.decoder_L4(x)))\n",
    "        x = self.decoder_R5(self.dropout(self.decoder_L5(x)))\n",
    "        x = self.decoder_R6(self.dropout(self.decoder_L6(x)))\n",
    "        x = self.decoder_R7(self.dropout(self.decoder_L7(x)))\n",
    "        x = self.decoder_R8(self.dropout(self.decoder_L8(x)))\n",
    "        x = self.decoder_R9(self.decoder_L9(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# init training network classes / architectures\n",
    "encoder_train = encoder()\n",
    "decoder_train = decoder()\n",
    "\n",
    "# push to cuda if cudnn is available\n",
    "if (torch.backends.cudnn.version() != None):\n",
    "    encoder_train = encoder().cuda()\n",
    "    decoder_train = decoder().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20180125-08:49:19] encoder architecture:\n",
      "\n",
      "encoder(\n",
      "  (dropout): Dropout(p=0.0, inplace)\n",
      "  (encoder_L1): Linear(in_features=401, out_features=512)\n",
      "  (encoder_R1): LeakyReLU(0.4, inplace)\n",
      "  (encoder_L2): Linear(in_features=512, out_features=256)\n",
      "  (encoder_R2): LeakyReLU(0.4, inplace)\n",
      "  (encoder_L3): Linear(in_features=256, out_features=128)\n",
      "  (encoder_R3): LeakyReLU(0.4, inplace)\n",
      "  (encoder_L4): Linear(in_features=128, out_features=64)\n",
      "  (encoder_R4): LeakyReLU(0.4, inplace)\n",
      "  (encoder_L5): Linear(in_features=64, out_features=32)\n",
      "  (encoder_R5): LeakyReLU(0.4, inplace)\n",
      "  (encoder_L6): Linear(in_features=32, out_features=16)\n",
      "  (encoder_R6): LeakyReLU(0.4, inplace)\n",
      "  (encoder_L7): Linear(in_features=16, out_features=8)\n",
      "  (encoder_R7): LeakyReLU(0.4, inplace)\n",
      "  (encoder_L8): Linear(in_features=8, out_features=4)\n",
      "  (encoder_R8): LeakyReLU(0.4, inplace)\n",
      "  (encoder_L9): Linear(in_features=4, out_features=3)\n",
      "  (encoder_R9): LeakyReLU(0.4, inplace)\n",
      ")\n",
      "\n",
      "[LOG 20180125-08:49:19] decoder architecture:\n",
      "\n",
      "decoder(\n",
      "  (dropout): Dropout(p=0.0, inplace)\n",
      "  (decoder_L1): Linear(in_features=3, out_features=4)\n",
      "  (decoder_R1): LeakyReLU(0.4, inplace)\n",
      "  (decoder_L2): Linear(in_features=4, out_features=8)\n",
      "  (decoder_R2): LeakyReLU(0.4, inplace)\n",
      "  (decoder_L3): Linear(in_features=8, out_features=16)\n",
      "  (decoder_R3): LeakyReLU(0.4, inplace)\n",
      "  (decoder_L4): Linear(in_features=16, out_features=32)\n",
      "  (decoder_R4): LeakyReLU(0.4, inplace)\n",
      "  (decoder_L5): Linear(in_features=32, out_features=64)\n",
      "  (decoder_R5): LeakyReLU(0.4, inplace)\n",
      "  (decoder_L6): Linear(in_features=64, out_features=128)\n",
      "  (decoder_R6): LeakyReLU(0.4, inplace)\n",
      "  (decoder_L7): Linear(in_features=128, out_features=256)\n",
      "  (decoder_R7): LeakyReLU(0.4, inplace)\n",
      "  (decoder_L8): Linear(in_features=256, out_features=512)\n",
      "  (decoder_R8): LeakyReLU(0.4, inplace)\n",
      "  (decoder_L9): Linear(in_features=512, out_features=401)\n",
      "  (decoder_R9): LeakyReLU(0.4, inplace)\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the initialized architectures\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] encoder architecture:\\n\\n{}\\n'.format(now, encoder_train))\n",
    "print('[LOG {}] decoder architecture:\\n\\n{}\\n'.format(now, decoder_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Financial Fraud Detection Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo -- Timur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20180125-08:49:39] encoded transactions of shape [307457/401] imported\n"
     ]
    }
   ],
   "source": [
    "# import original and encoded transactions\n",
    "ori_dataset = pd.read_csv(\"./data/transactions.csv\", sep=\",\", header=0, encoding=\"utf-8\")\n",
    "enc_dataset = pd.read_csv(\"./data/enc_transactions.csv\", sep=\",\", header=0, encoding=\"utf-8\").astype(float)\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print(\"[LOG {}] encoded transactions of shape [{}/{}] imported\".format(str(now), str(enc_dataset.shape[0]), str(enc_dataset.shape[1])))\n",
    "\n",
    "torch_dataset = torch.from_numpy(enc_dataset.values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_Key</th>\n",
       "      <th>Mapping_Key</th>\n",
       "      <th>NetValueInDocumentCurrency</th>\n",
       "      <th>TaxAmountInDocumentCurrency</th>\n",
       "      <th>ProductCode_Key</th>\n",
       "      <th>AccountID_Key</th>\n",
       "      <th>CurrencyCode_Key</th>\n",
       "      <th>TaxCode_Key</th>\n",
       "      <th>CompanyKey_Key</th>\n",
       "      <th>ShipToCountry_Key</th>\n",
       "      <th>ShipFromCountry_Key</th>\n",
       "      <th>CustomerID_ShipTo_Key</th>\n",
       "      <th>CustomerID_BillTo_Key</th>\n",
       "      <th>Artifical_Outlier</th>\n",
       "      <th>Reconstruction_Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>999999.0</td>\n",
       "      <td>50000.0000</td>\n",
       "      <td>Aplha</td>\n",
       "      <td>Bravo</td>\n",
       "      <td>Tango</td>\n",
       "      <td>Fruti</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>India</td>\n",
       "      <td>Anant</td>\n",
       "      <td>Marco</td>\n",
       "      <td>Bermuda</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>598434.0</td>\n",
       "      <td>88000.0000</td>\n",
       "      <td>Zori</td>\n",
       "      <td>Account</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>Python</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>details</td>\n",
       "      <td>?ro7ekt</td>\n",
       "      <td>placed</td>\n",
       "      <td>SOurdf</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>450864.0</td>\n",
       "      <td>70005.0000</td>\n",
       "      <td>Author</td>\n",
       "      <td>Book</td>\n",
       "      <td>What??</td>\n",
       "      <td>DUH!!</td>\n",
       "      <td>I</td>\n",
       "      <td>Hate</td>\n",
       "      <td>these</td>\n",
       "      <td>Anamolies</td>\n",
       "      <td>Detection</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108888.0</td>\n",
       "      <td>28000.0000</td>\n",
       "      <td>top</td>\n",
       "      <td>what!</td>\n",
       "      <td>AyCaramba!</td>\n",
       "      <td>PwC</td>\n",
       "      <td>FTS</td>\n",
       "      <td>=Great</td>\n",
       "      <td>Maybe?</td>\n",
       "      <td>Artificial</td>\n",
       "      <td>Outliers</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>233355.0</td>\n",
       "      <td>4514.5617</td>\n",
       "      <td>Java</td>\n",
       "      <td>Progrramming</td>\n",
       "      <td>Sucks</td>\n",
       "      <td>really.</td>\n",
       "      <td>Go</td>\n",
       "      <td>Kalrsruhe!</td>\n",
       "      <td>I</td>\n",
       "      <td>am</td>\n",
       "      <td>dumb</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44348.0</td>\n",
       "      <td>2691.0000</td>\n",
       "      <td>Stuttgart/Karlsruhe</td>\n",
       "      <td>(pm/rl)</td>\n",
       "      <td>Die</td>\n",
       "      <td>Forscher</td>\n",
       "      <td>Johannes</td>\n",
       "      <td>Gescher</td>\n",
       "      <td>und</td>\n",
       "      <td>Katrin</td>\n",
       "      <td>Klink</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>265100.0</td>\n",
       "      <td>4083.0000</td>\n",
       "      <td>Für</td>\n",
       "      <td>die</td>\n",
       "      <td>Ausschreibung</td>\n",
       "      <td>„Fellowships</td>\n",
       "      <td>für</td>\n",
       "      <td>Innovationen</td>\n",
       "      <td>in</td>\n",
       "      <td>der</td>\n",
       "      <td>Hochschullehre“</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>102940.0</td>\n",
       "      <td>2026.0000</td>\n",
       "      <td>Christoph</td>\n",
       "      <td>Dahl,</td>\n",
       "      <td>Geschäftsführer</td>\n",
       "      <td>der</td>\n",
       "      <td>Baden-Württemberg</td>\n",
       "      <td>Stiftung,</td>\n",
       "      <td>lobte</td>\n",
       "      <td>die</td>\n",
       "      <td>Arbeit</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104639.0</td>\n",
       "      <td>4137.0000</td>\n",
       "      <td>vom</td>\n",
       "      <td>Karlsruher</td>\n",
       "      <td>Institut</td>\n",
       "      <td>für</td>\n",
       "      <td>Technologie</td>\n",
       "      <td>(KIT)</td>\n",
       "      <td>sind</td>\n",
       "      <td>für</td>\n",
       "      <td>ihre</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>815721.0</td>\n",
       "      <td>2806.0000</td>\n",
       "      <td>der</td>\n",
       "      <td>Baden-Württemberg</td>\n",
       "      <td>Stiftung</td>\n",
       "      <td>sind</td>\n",
       "      <td>rund</td>\n",
       "      <td>120</td>\n",
       "      <td>Anträge</td>\n",
       "      <td>eingegangen.</td>\n",
       "      <td>Aus</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_Key Mapping_Key  NetValueInDocumentCurrency  \\\n",
       "0       1         NaN                    999999.0   \n",
       "1       2         NaN                    598434.0   \n",
       "2       3         NaN                    450864.0   \n",
       "3       4         NaN                    108888.0   \n",
       "4       5         NaN                    233355.0   \n",
       "5       6         NaN                     44348.0   \n",
       "6       7         NaN                    265100.0   \n",
       "7       8         NaN                    102940.0   \n",
       "8       9         NaN                    104639.0   \n",
       "9      10         NaN                    815721.0   \n",
       "\n",
       "   TaxAmountInDocumentCurrency      ProductCode_Key      AccountID_Key  \\\n",
       "0                   50000.0000                Aplha              Bravo   \n",
       "1                   88000.0000                 Zori            Account   \n",
       "2                   70005.0000               Author               Book   \n",
       "3                   28000.0000                  top              what!   \n",
       "4                    4514.5617                 Java       Progrramming   \n",
       "5                    2691.0000  Stuttgart/Karlsruhe            (pm/rl)   \n",
       "6                    4083.0000                  Für                die   \n",
       "7                    2026.0000            Christoph              Dahl,   \n",
       "8                    4137.0000                  vom         Karlsruher   \n",
       "9                    2806.0000                  der  Baden-Württemberg   \n",
       "\n",
       "  CurrencyCode_Key   TaxCode_Key     CompanyKey_Key ShipToCountry_Key  \\\n",
       "0            Tango         Fruti              Kenya             India   \n",
       "1      Categorical        Python          Indonesia           details   \n",
       "2           What??         DUH!!                  I              Hate   \n",
       "3       AyCaramba!           PwC                FTS            =Great   \n",
       "4            Sucks       really.                 Go        Kalrsruhe!   \n",
       "5              Die      Forscher           Johannes           Gescher   \n",
       "6    Ausschreibung  „Fellowships                für      Innovationen   \n",
       "7  Geschäftsführer           der  Baden-Württemberg         Stiftung,   \n",
       "8         Institut           für        Technologie             (KIT)   \n",
       "9         Stiftung          sind               rund               120   \n",
       "\n",
       "  ShipFromCountry_Key CustomerID_ShipTo_Key CustomerID_BillTo_Key  \\\n",
       "0               Anant                 Marco               Bermuda   \n",
       "1             ?ro7ekt                placed                SOurdf   \n",
       "2               these             Anamolies             Detection   \n",
       "3              Maybe?            Artificial              Outliers   \n",
       "4                   I                    am                  dumb   \n",
       "5                 und                Katrin                 Klink   \n",
       "6                  in                   der       Hochschullehre“   \n",
       "7               lobte                   die                Arbeit   \n",
       "8                sind                   für                  ihre   \n",
       "9             Anträge          eingegangen.                   Aus   \n",
       "\n",
       "   Artifical_Outlier  Reconstruction_Error  \n",
       "0                  1                   NaN  \n",
       "1                  1                   NaN  \n",
       "2                  1                   NaN  \n",
       "3                  1                   NaN  \n",
       "4                  1                   NaN  \n",
       "5                  1                   NaN  \n",
       "6                  1                   NaN  \n",
       "7                  1                   NaN  \n",
       "8                  1                   NaN  \n",
       "9                  1                   NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = [\"AccountID_Key\", \"CurrencyCode_Key\", \"TaxCode_Key\", \"CompanyKey_Key\", \"ShipToCountry_Key\", \"ShipFromCountry_Key\"]\n",
    "ranges = [0, 62+1, 121+1, 183+1, 234+1, 349+1, 400+1]\n",
    "\n",
    "# init training results\n",
    "columns = [\"timestamp\", \"node\", \"seed\", \"architecture\", \"epoch\", \"rec_loss\", \"roc_auc\", \"anomalies\", \"normalies\", \"anomalies_s\", \"normalies_s\", \"max_threshold\", \"max_tpr_s\", \"max_fpr_s\", \"precision\", \"recall\", \"f1_score\", \"fpr\", \"tpr\", \"thresholds\"]\n",
    "evaluations = [\"err_\" + str(element) for element in range(0, (len(features))+1)]\n",
    "columns.extend(evaluations)\n",
    "evaluations = [\"ano_c1_\" + str(element) for element in range(0, (len(features))+1)]\n",
    "columns.extend(evaluations)\n",
    "evaluations = [\"ano_c2_\" + str(element) for element in range(0, (len(features))+1)]\n",
    "columns.extend(evaluations)\n",
    "evaluation_results = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05. Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo - Timur and Marco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20180125-08_49_45] training status, epoch: [0001/0010], batch: 0100, loss: 0.0274375156\n",
      "[LOG 20180125-08_49_49] training status, epoch: [0001/0010], batch: 0200, loss: 0.0174645148\n",
      "[LOG 20180125-08_49_52] training status, epoch: [0001/0010], batch: 0300, loss: 0.0082735242\n",
      "[LOG 20180125-08_49_55] training status, epoch: [0001/0010], batch: 0400, loss: 0.0069946530\n",
      "[LOG 20180125-08_49_58] training status, epoch: [0001/0010], batch: 0500, loss: 0.0053769667\n",
      "[LOG 20180125-08_50_01] training status, epoch: [0001/0010], batch: 0600, loss: 0.0069966190\n",
      "[LOG 20180125-08_50_04] training status, epoch: [0001/0010], batch: 0700, loss: 0.0053451452\n",
      "[LOG 20180125-08_50_07] training status, epoch: [0001/0010], batch: 0800, loss: 0.0035927675\n",
      "[LOG 20180125-08_50_11] training status, epoch: [0001/0010], batch: 0900, loss: 0.0029478113\n",
      "[LOG 20180125-08_50_15] training status, epoch: [0001/0010], batch: 1000, loss: 0.0028416477\n",
      "[LOG 20180125-08_50_19] training status, epoch: [0001/0010], batch: 1100, loss: 0.0031440654\n",
      "[LOG 20180125-08_50_22] training status, epoch: [0001/0010], batch: 1200, loss: 0.0053522657\n",
      "[LOG 20180125-08_50_25] training status, epoch: [0001/0010], batch: 1300, loss: 0.0030031146\n",
      "[LOG 20180125-08_50_28] training status, epoch: [0001/0010], batch: 1400, loss: 0.0048323041\n",
      "[LOG 20180125-08_50_31] training status, epoch: [0001/0010], batch: 1500, loss: 0.0033528353\n",
      "[LOG 20180125-08_50_35] training status, epoch: [0001/0010], batch: 1600, loss: 0.0026675025\n",
      "[LOG 20180125-08_50_38] training status, epoch: [0001/0010], batch: 1700, loss: 0.0032466999\n",
      "[LOG 20180125-08_50_42] training status, epoch: [0001/0010], batch: 1800, loss: 0.0020601579\n",
      "[LOG 20180125-08_50_45] training status, epoch: [0001/0010], batch: 1900, loss: 0.0022864621\n",
      "[LOG 20180125-08_50_49] training status, epoch: [0001/0010], batch: 2000, loss: 0.0033078406\n",
      "[LOG 20180125-08_50_53] training status, epoch: [0001/0010], batch: 2100, loss: 0.0020727885\n",
      "[LOG 20180125-08_50_58] training status, epoch: [0001/0010], batch: 2200, loss: 0.0025662226\n",
      "[LOG 20180125-08_51_01] training status, epoch: [0001/0010], batch: 2300, loss: 0.0018359601\n",
      "[LOG 20180125-08_51_04] training status, epoch: [0001/0010], batch: 2400, loss: 0.0024230301\n",
      "[LOG 20180125-08_51_07] training status, epoch: [0002/0010], batch: 0100, loss: 0.0019330899\n",
      "[LOG 20180125-08_51_10] training status, epoch: [0002/0010], batch: 0200, loss: 0.0016046999\n",
      "[LOG 20180125-08_51_13] training status, epoch: [0002/0010], batch: 0300, loss: 0.0020069813\n",
      "[LOG 20180125-08_51_17] training status, epoch: [0002/0010], batch: 0400, loss: 0.0029661909\n",
      "[LOG 20180125-08_51_20] training status, epoch: [0002/0010], batch: 0500, loss: 0.0014648723\n",
      "[LOG 20180125-08_51_23] training status, epoch: [0002/0010], batch: 0600, loss: 0.0023244910\n",
      "[LOG 20180125-08_51_27] training status, epoch: [0002/0010], batch: 0700, loss: 0.0010045266\n",
      "[LOG 20180125-08_51_32] training status, epoch: [0002/0010], batch: 0800, loss: 0.0011724480\n",
      "[LOG 20180125-08_51_37] training status, epoch: [0002/0010], batch: 0900, loss: 0.0011225803\n",
      "[LOG 20180125-08_51_41] training status, epoch: [0002/0010], batch: 1000, loss: 0.0282042772\n",
      "[LOG 20180125-08_51_45] training status, epoch: [0002/0010], batch: 1100, loss: 0.0036199160\n",
      "[LOG 20180125-08_51_48] training status, epoch: [0002/0010], batch: 1200, loss: 0.0034995975\n",
      "[LOG 20180125-08_51_52] training status, epoch: [0002/0010], batch: 1300, loss: 0.0025672887\n",
      "[LOG 20180125-08_51_56] training status, epoch: [0002/0010], batch: 1400, loss: 0.0057919924\n",
      "[LOG 20180125-08_51_59] training status, epoch: [0002/0010], batch: 1500, loss: 0.0016977156\n",
      "[LOG 20180125-08_52_03] training status, epoch: [0002/0010], batch: 1600, loss: 0.0021700256\n",
      "[LOG 20180125-08_52_06] training status, epoch: [0002/0010], batch: 1700, loss: 0.0018524533\n",
      "[LOG 20180125-08_52_10] training status, epoch: [0002/0010], batch: 1800, loss: 0.0013535742\n",
      "[LOG 20180125-08_52_14] training status, epoch: [0002/0010], batch: 1900, loss: 0.0016319701\n",
      "[LOG 20180125-08_52_17] training status, epoch: [0002/0010], batch: 2000, loss: 0.0016563113\n",
      "[LOG 20180125-08_52_20] training status, epoch: [0002/0010], batch: 2100, loss: 0.0018765564\n",
      "[LOG 20180125-08_52_24] training status, epoch: [0002/0010], batch: 2200, loss: 0.0012716266\n",
      "[LOG 20180125-08_52_28] training status, epoch: [0002/0010], batch: 2300, loss: 0.0074234176\n",
      "[LOG 20180125-08_52_31] training status, epoch: [0002/0010], batch: 2400, loss: 0.0029455344\n",
      "[LOG 20180125-08_52_35] training status, epoch: [0003/0010], batch: 0100, loss: 0.0026847436\n",
      "[LOG 20180125-08_52_38] training status, epoch: [0003/0010], batch: 0200, loss: 0.0009298215\n",
      "[LOG 20180125-08_52_42] training status, epoch: [0003/0010], batch: 0300, loss: 0.0013461261\n",
      "[LOG 20180125-08_52_45] training status, epoch: [0003/0010], batch: 0400, loss: 0.0009335273\n",
      "[LOG 20180125-08_52_48] training status, epoch: [0003/0010], batch: 0500, loss: 0.0012866688\n",
      "[LOG 20180125-08_52_52] training status, epoch: [0003/0010], batch: 0600, loss: 0.0013896143\n",
      "[LOG 20180125-08_52_55] training status, epoch: [0003/0010], batch: 0700, loss: 0.0008317492\n",
      "[LOG 20180125-08_52_58] training status, epoch: [0003/0010], batch: 0800, loss: 0.0010925093\n",
      "[LOG 20180125-08_53_02] training status, epoch: [0003/0010], batch: 0900, loss: 0.0008686027\n",
      "[LOG 20180125-08_53_05] training status, epoch: [0003/0010], batch: 1000, loss: 0.0008528739\n",
      "[LOG 20180125-08_53_08] training status, epoch: [0003/0010], batch: 1100, loss: 0.0010120494\n",
      "[LOG 20180125-08_53_11] training status, epoch: [0003/0010], batch: 1200, loss: 0.0028016090\n",
      "[LOG 20180125-08_53_14] training status, epoch: [0003/0010], batch: 1300, loss: 0.0013651173\n",
      "[LOG 20180125-08_53_17] training status, epoch: [0003/0010], batch: 1400, loss: 0.0011890802\n",
      "[LOG 20180125-08_53_20] training status, epoch: [0003/0010], batch: 1500, loss: 0.0034422164\n",
      "[LOG 20180125-08_53_23] training status, epoch: [0003/0010], batch: 1600, loss: 0.0009367354\n",
      "[LOG 20180125-08_53_27] training status, epoch: [0003/0010], batch: 1700, loss: 0.0020428624\n",
      "[LOG 20180125-08_53_30] training status, epoch: [0003/0010], batch: 1800, loss: 0.0007934551\n",
      "[LOG 20180125-08_53_33] training status, epoch: [0003/0010], batch: 1900, loss: 0.0017787780\n",
      "[LOG 20180125-08_53_36] training status, epoch: [0003/0010], batch: 2000, loss: 0.0017025537\n",
      "[LOG 20180125-08_53_39] training status, epoch: [0003/0010], batch: 2100, loss: 0.0014214892\n",
      "[LOG 20180125-08_53_42] training status, epoch: [0003/0010], batch: 2200, loss: 0.0010465509\n",
      "[LOG 20180125-08_53_45] training status, epoch: [0003/0010], batch: 2300, loss: 0.0033934030\n",
      "[LOG 20180125-08_53_48] training status, epoch: [0003/0010], batch: 2400, loss: 0.0005189593\n",
      "[LOG 20180125-08_53_51] training status, epoch: [0004/0010], batch: 0100, loss: 0.0043990402\n",
      "[LOG 20180125-08_53_55] training status, epoch: [0004/0010], batch: 0200, loss: 0.0009984657\n",
      "[LOG 20180125-08_53_57] training status, epoch: [0004/0010], batch: 0300, loss: 0.0020035512\n",
      "[LOG 20180125-08_54_01] training status, epoch: [0004/0010], batch: 0400, loss: 0.0011332071\n",
      "[LOG 20180125-08_54_04] training status, epoch: [0004/0010], batch: 0500, loss: 0.0007451448\n",
      "[LOG 20180125-08_54_07] training status, epoch: [0004/0010], batch: 0600, loss: 0.0014248512\n",
      "[LOG 20180125-08_54_10] training status, epoch: [0004/0010], batch: 0700, loss: 0.0009189575\n",
      "[LOG 20180125-08_54_13] training status, epoch: [0004/0010], batch: 0800, loss: 0.0010643667\n",
      "[LOG 20180125-08_54_17] training status, epoch: [0004/0010], batch: 0900, loss: 0.0013420514\n",
      "[LOG 20180125-08_54_20] training status, epoch: [0004/0010], batch: 1000, loss: 0.0011173339\n",
      "[LOG 20180125-08_54_24] training status, epoch: [0004/0010], batch: 1100, loss: 0.0007209727\n",
      "[LOG 20180125-08_54_27] training status, epoch: [0004/0010], batch: 1200, loss: 0.0019370556\n",
      "[LOG 20180125-08_54_30] training status, epoch: [0004/0010], batch: 1300, loss: 0.0008676780\n",
      "[LOG 20180125-08_54_33] training status, epoch: [0004/0010], batch: 1400, loss: 0.0010766266\n",
      "[LOG 20180125-08_54_36] training status, epoch: [0004/0010], batch: 1500, loss: 0.0011043877\n",
      "[LOG 20180125-08_54_39] training status, epoch: [0004/0010], batch: 1600, loss: 0.0009159778\n",
      "[LOG 20180125-08_54_42] training status, epoch: [0004/0010], batch: 1700, loss: 0.0003905415\n",
      "[LOG 20180125-08_54_45] training status, epoch: [0004/0010], batch: 1800, loss: 0.0010546020\n",
      "[LOG 20180125-08_54_49] training status, epoch: [0004/0010], batch: 1900, loss: 0.0006326878\n",
      "[LOG 20180125-08_54_52] training status, epoch: [0004/0010], batch: 2000, loss: 0.0009984174\n",
      "[LOG 20180125-08_54_54] training status, epoch: [0004/0010], batch: 2100, loss: 0.0033460835\n",
      "[LOG 20180125-08_54_57] training status, epoch: [0004/0010], batch: 2200, loss: 0.0007314572\n",
      "[LOG 20180125-08_55_00] training status, epoch: [0004/0010], batch: 2300, loss: 0.0004911198\n",
      "[LOG 20180125-08_55_03] training status, epoch: [0004/0010], batch: 2400, loss: 0.0035424577\n",
      "[LOG 20180125-08_55_06] training status, epoch: [0005/0010], batch: 0100, loss: 0.0012994106\n",
      "[LOG 20180125-08_55_09] training status, epoch: [0005/0010], batch: 0200, loss: 0.0006478245\n",
      "[LOG 20180125-08_55_12] training status, epoch: [0005/0010], batch: 0300, loss: 0.0012047130\n",
      "[LOG 20180125-08_55_15] training status, epoch: [0005/0010], batch: 0400, loss: 0.0006068120\n",
      "[LOG 20180125-08_55_18] training status, epoch: [0005/0010], batch: 0500, loss: 0.0003474320\n",
      "[LOG 20180125-08_55_22] training status, epoch: [0005/0010], batch: 0600, loss: 0.0006517596\n",
      "[LOG 20180125-08_55_25] training status, epoch: [0005/0010], batch: 0700, loss: 0.0010042136\n",
      "[LOG 20180125-08_55_28] training status, epoch: [0005/0010], batch: 0800, loss: 0.0004479067\n",
      "[LOG 20180125-08_55_32] training status, epoch: [0005/0010], batch: 0900, loss: 0.0001818787\n",
      "[LOG 20180125-08_55_35] training status, epoch: [0005/0010], batch: 1000, loss: 0.0010754176\n",
      "[LOG 20180125-08_55_38] training status, epoch: [0005/0010], batch: 1100, loss: 0.0012345222\n",
      "[LOG 20180125-08_55_41] training status, epoch: [0005/0010], batch: 1200, loss: 0.0004680627\n",
      "[LOG 20180125-08_55_44] training status, epoch: [0005/0010], batch: 1300, loss: 0.0005482768\n",
      "[LOG 20180125-08_55_48] training status, epoch: [0005/0010], batch: 1400, loss: 0.0002346474\n",
      "[LOG 20180125-08_55_51] training status, epoch: [0005/0010], batch: 1500, loss: 0.0004127454\n",
      "[LOG 20180125-08_55_56] training status, epoch: [0005/0010], batch: 1600, loss: 0.0004589100\n",
      "[LOG 20180125-08_56_01] training status, epoch: [0005/0010], batch: 1700, loss: 0.0006561905\n",
      "[LOG 20180125-08_56_06] training status, epoch: [0005/0010], batch: 1800, loss: 0.0024664316\n",
      "[LOG 20180125-08_56_09] training status, epoch: [0005/0010], batch: 1900, loss: 0.0005095351\n",
      "[LOG 20180125-08_56_12] training status, epoch: [0005/0010], batch: 2000, loss: 0.0005191066\n",
      "[LOG 20180125-08_56_16] training status, epoch: [0005/0010], batch: 2100, loss: 0.0002165884\n",
      "[LOG 20180125-08_56_18] training status, epoch: [0005/0010], batch: 2200, loss: 0.0004673045\n",
      "[LOG 20180125-08_56_21] training status, epoch: [0005/0010], batch: 2300, loss: 0.0017984837\n",
      "[LOG 20180125-08_56_24] training status, epoch: [0005/0010], batch: 2400, loss: 0.0001738707\n",
      "[LOG 20180125-08_56_28] training status, epoch: [0006/0010], batch: 0100, loss: 0.0004471997\n",
      "[LOG 20180125-08_56_31] training status, epoch: [0006/0010], batch: 0200, loss: 0.0005410252\n",
      "[LOG 20180125-08_56_35] training status, epoch: [0006/0010], batch: 0300, loss: 0.0004624817\n",
      "[LOG 20180125-08_56_38] training status, epoch: [0006/0010], batch: 0400, loss: 0.0034849641\n",
      "[LOG 20180125-08_56_42] training status, epoch: [0006/0010], batch: 0500, loss: 0.0002321665\n",
      "[LOG 20180125-08_56_46] training status, epoch: [0006/0010], batch: 0600, loss: 0.0010704447\n",
      "[LOG 20180125-08_56_49] training status, epoch: [0006/0010], batch: 0700, loss: 0.0002273332\n",
      "[LOG 20180125-08_56_52] training status, epoch: [0006/0010], batch: 0800, loss: 0.0005597641\n",
      "[LOG 20180125-08_56_55] training status, epoch: [0006/0010], batch: 0900, loss: 0.0001868720\n",
      "[LOG 20180125-08_56_58] training status, epoch: [0006/0010], batch: 1000, loss: 0.0001718643\n",
      "[LOG 20180125-08_57_01] training status, epoch: [0006/0010], batch: 1100, loss: 0.0000954077\n",
      "[LOG 20180125-08_57_04] training status, epoch: [0006/0010], batch: 1200, loss: 0.0003358622\n",
      "[LOG 20180125-08_57_09] training status, epoch: [0006/0010], batch: 1300, loss: 0.0005820764\n",
      "[LOG 20180125-08_57_12] training status, epoch: [0006/0010], batch: 1400, loss: 0.0003904425\n",
      "[LOG 20180125-08_57_16] training status, epoch: [0006/0010], batch: 1500, loss: 0.0002713226\n",
      "[LOG 20180125-08_57_20] training status, epoch: [0006/0010], batch: 1600, loss: 0.0487065762\n",
      "[LOG 20180125-08_57_23] training status, epoch: [0006/0010], batch: 1700, loss: 0.0031195609\n",
      "[LOG 20180125-08_57_28] training status, epoch: [0006/0010], batch: 1800, loss: 0.0016993735\n",
      "[LOG 20180125-08_57_34] training status, epoch: [0006/0010], batch: 1900, loss: 0.0011091320\n",
      "[LOG 20180125-08_57_41] training status, epoch: [0006/0010], batch: 2000, loss: 0.0015021183\n",
      "[LOG 20180125-08_57_46] training status, epoch: [0006/0010], batch: 2100, loss: 0.0019254404\n",
      "[LOG 20180125-08_57_50] training status, epoch: [0006/0010], batch: 2200, loss: 0.0005105078\n",
      "[LOG 20180125-08_57_53] training status, epoch: [0006/0010], batch: 2300, loss: 0.0006418703\n",
      "[LOG 20180125-08_57_56] training status, epoch: [0006/0010], batch: 2400, loss: 0.0006101452\n",
      "[LOG 20180125-08_57_59] training status, epoch: [0007/0010], batch: 0100, loss: 0.0002588030\n",
      "[LOG 20180125-08_58_02] training status, epoch: [0007/0010], batch: 0200, loss: 0.0005787085\n",
      "[LOG 20180125-08_58_05] training status, epoch: [0007/0010], batch: 0300, loss: 0.0005682099\n",
      "[LOG 20180125-08_58_09] training status, epoch: [0007/0010], batch: 0400, loss: 0.0002902539\n",
      "[LOG 20180125-08_58_12] training status, epoch: [0007/0010], batch: 0500, loss: 0.0003902810\n",
      "[LOG 20180125-08_58_15] training status, epoch: [0007/0010], batch: 0600, loss: 0.0005349085\n",
      "[LOG 20180125-08_58_18] training status, epoch: [0007/0010], batch: 0700, loss: 0.0001494797\n",
      "[LOG 20180125-08_58_21] training status, epoch: [0007/0010], batch: 0800, loss: 0.0005832075\n",
      "[LOG 20180125-08_58_24] training status, epoch: [0007/0010], batch: 0900, loss: 0.0001894725\n",
      "[LOG 20180125-08_58_27] training status, epoch: [0007/0010], batch: 1000, loss: 0.0006393894\n",
      "[LOG 20180125-08_58_30] training status, epoch: [0007/0010], batch: 1100, loss: 0.0003173688\n",
      "[LOG 20180125-08_58_36] training status, epoch: [0007/0010], batch: 1200, loss: 0.0000616539\n",
      "[LOG 20180125-08_58_41] training status, epoch: [0007/0010], batch: 1300, loss: 0.0003332700\n",
      "[LOG 20180125-08_58_50] training status, epoch: [0007/0010], batch: 1400, loss: 0.0009177382\n",
      "[LOG 20180125-08_58_55] training status, epoch: [0007/0010], batch: 1500, loss: 0.0003318211\n",
      "[LOG 20180125-08_59_01] training status, epoch: [0007/0010], batch: 1600, loss: 0.0003544819\n",
      "[LOG 20180125-08_59_07] training status, epoch: [0007/0010], batch: 1700, loss: 0.0011775072\n",
      "[LOG 20180125-08_59_11] training status, epoch: [0007/0010], batch: 1800, loss: 0.0001649549\n",
      "[LOG 20180125-08_59_16] training status, epoch: [0007/0010], batch: 1900, loss: 0.0017720069\n",
      "[LOG 20180125-08_59_24] training status, epoch: [0007/0010], batch: 2000, loss: 0.0006272951\n",
      "[LOG 20180125-08_59_31] training status, epoch: [0007/0010], batch: 2100, loss: 0.0005318106\n",
      "[LOG 20180125-08_59_36] training status, epoch: [0007/0010], batch: 2200, loss: 0.0005169311\n",
      "[LOG 20180125-08_59_39] training status, epoch: [0007/0010], batch: 2300, loss: 0.0001219997\n",
      "[LOG 20180125-08_59_42] training status, epoch: [0007/0010], batch: 2400, loss: 0.0006383322\n",
      "[LOG 20180125-08_59_45] training status, epoch: [0008/0010], batch: 0100, loss: 0.0002629394\n",
      "[LOG 20180125-08_59_48] training status, epoch: [0008/0010], batch: 0200, loss: 0.0002012643\n",
      "[LOG 20180125-08_59_51] training status, epoch: [0008/0010], batch: 0300, loss: 0.0000924392\n",
      "[LOG 20180125-08_59_54] training status, epoch: [0008/0010], batch: 0400, loss: 0.0036832951\n",
      "[LOG 20180125-08_59_57] training status, epoch: [0008/0010], batch: 0500, loss: 0.0007491189\n",
      "[LOG 20180125-09_00_00] training status, epoch: [0008/0010], batch: 0600, loss: 0.0001524733\n",
      "[LOG 20180125-09_00_03] training status, epoch: [0008/0010], batch: 0700, loss: 0.0001844403\n",
      "[LOG 20180125-09_00_08] training status, epoch: [0008/0010], batch: 0800, loss: 0.0002715847\n",
      "[LOG 20180125-09_00_14] training status, epoch: [0008/0010], batch: 0900, loss: 0.0001893233\n",
      "[LOG 20180125-09_00_16] training status, epoch: [0008/0010], batch: 1000, loss: 0.0001813047\n",
      "[LOG 20180125-09_00_20] training status, epoch: [0008/0010], batch: 1100, loss: 0.0072865258\n",
      "[LOG 20180125-09_00_23] training status, epoch: [0008/0010], batch: 1200, loss: 0.0025954647\n",
      "[LOG 20180125-09_00_26] training status, epoch: [0008/0010], batch: 1300, loss: 0.0001745684\n",
      "[LOG 20180125-09_00_29] training status, epoch: [0008/0010], batch: 1400, loss: 0.0002251026\n",
      "[LOG 20180125-09_00_32] training status, epoch: [0008/0010], batch: 1500, loss: 0.0003018193\n",
      "[LOG 20180125-09_00_40] training status, epoch: [0008/0010], batch: 1600, loss: 0.0002554857\n",
      "[LOG 20180125-09_00_45] training status, epoch: [0008/0010], batch: 1700, loss: 0.0002956305\n",
      "[LOG 20180125-09_00_49] training status, epoch: [0008/0010], batch: 1800, loss: 0.0020967696\n",
      "[LOG 20180125-09_00_53] training status, epoch: [0008/0010], batch: 1900, loss: 0.0018149182\n",
      "[LOG 20180125-09_00_56] training status, epoch: [0008/0010], batch: 2000, loss: 0.0003221947\n",
      "[LOG 20180125-09_00_59] training status, epoch: [0008/0010], batch: 2100, loss: 0.0002884427\n",
      "[LOG 20180125-09_01_03] training status, epoch: [0008/0010], batch: 2200, loss: 0.0002646270\n",
      "[LOG 20180125-09_01_07] training status, epoch: [0008/0010], batch: 2300, loss: 0.0012084191\n",
      "[LOG 20180125-09_01_11] training status, epoch: [0008/0010], batch: 2400, loss: 0.0004670013\n",
      "[LOG 20180125-09_01_14] training status, epoch: [0009/0010], batch: 0100, loss: 0.0001849020\n",
      "[LOG 20180125-09_01_17] training status, epoch: [0009/0010], batch: 0200, loss: 0.0012240333\n",
      "[LOG 20180125-09_01_21] training status, epoch: [0009/0010], batch: 0300, loss: 0.0004276188\n",
      "[LOG 20180125-09_01_24] training status, epoch: [0009/0010], batch: 0400, loss: 0.0010954486\n",
      "[LOG 20180125-09_01_27] training status, epoch: [0009/0010], batch: 0500, loss: 0.0005693024\n",
      "[LOG 20180125-09_01_31] training status, epoch: [0009/0010], batch: 0600, loss: 0.0001728542\n",
      "[LOG 20180125-09_01_34] training status, epoch: [0009/0010], batch: 0700, loss: 0.0003578528\n",
      "[LOG 20180125-09_01_38] training status, epoch: [0009/0010], batch: 0800, loss: 0.0004735102\n",
      "[LOG 20180125-09_02_25] training status, epoch: [0009/0010], batch: 0900, loss: 0.0001963902\n",
      "[LOG 20180125-09_02_36] training status, epoch: [0009/0010], batch: 1000, loss: 0.0001520739\n",
      "[LOG 20180125-09_02_48] training status, epoch: [0009/0010], batch: 1100, loss: 0.0001209207\n",
      "[LOG 20180125-09_03_01] training status, epoch: [0009/0010], batch: 1200, loss: 0.0001574015\n",
      "[LOG 20180125-09_03_13] training status, epoch: [0009/0010], batch: 1300, loss: 0.0001186871\n",
      "[LOG 20180125-09_03_23] training status, epoch: [0009/0010], batch: 1400, loss: 0.0001288817\n",
      "[LOG 20180125-09_03_27] training status, epoch: [0009/0010], batch: 1500, loss: 0.0000851892\n",
      "[LOG 20180125-09_03_31] training status, epoch: [0009/0010], batch: 1600, loss: 0.0005594472\n",
      "[LOG 20180125-09_03_36] training status, epoch: [0009/0010], batch: 1700, loss: 0.0004390227\n",
      "[LOG 20180125-09_03_39] training status, epoch: [0009/0010], batch: 1800, loss: 0.0003028424\n",
      "[LOG 20180125-09_03_42] training status, epoch: [0009/0010], batch: 1900, loss: 0.0001635571\n",
      "[LOG 20180125-09_03_45] training status, epoch: [0009/0010], batch: 2000, loss: 0.0002126034\n",
      "[LOG 20180125-09_03_51] training status, epoch: [0009/0010], batch: 2100, loss: 0.0004744544\n",
      "[LOG 20180125-09_03_57] training status, epoch: [0009/0010], batch: 2200, loss: 0.0002798030\n",
      "[LOG 20180125-09_04_03] training status, epoch: [0009/0010], batch: 2300, loss: 0.0003294187\n",
      "[LOG 20180125-09_04_13] training status, epoch: [0009/0010], batch: 2400, loss: 0.0003564427\n",
      "[LOG 20180125-09_04_25] training status, epoch: [0010/0010], batch: 0100, loss: 0.0002615184\n",
      "[LOG 20180125-09_04_41] training status, epoch: [0010/0010], batch: 0200, loss: 0.0003077175\n",
      "[LOG 20180125-09_04_47] training status, epoch: [0010/0010], batch: 0300, loss: 0.0004665465\n",
      "[LOG 20180125-09_04_57] training status, epoch: [0010/0010], batch: 0400, loss: 0.0002259590\n",
      "[LOG 20180125-09_05_00] training status, epoch: [0010/0010], batch: 0500, loss: 0.0002467828\n",
      "[LOG 20180125-09_05_07] training status, epoch: [0010/0010], batch: 0600, loss: 0.0002773034\n",
      "[LOG 20180125-09_05_19] training status, epoch: [0010/0010], batch: 0700, loss: 0.0002498262\n",
      "[LOG 20180125-09_05_30] training status, epoch: [0010/0010], batch: 0800, loss: 0.0001687153\n",
      "[LOG 20180125-09_05_40] training status, epoch: [0010/0010], batch: 0900, loss: 0.0001754687\n",
      "[LOG 20180125-09_05_46] training status, epoch: [0010/0010], batch: 1000, loss: 0.0006514696\n",
      "[LOG 20180125-09_05_55] training status, epoch: [0010/0010], batch: 1100, loss: 0.0008656787\n",
      "[LOG 20180125-09_06_01] training status, epoch: [0010/0010], batch: 1200, loss: 0.0007684819\n",
      "[LOG 20180125-09_06_06] training status, epoch: [0010/0010], batch: 1300, loss: 0.0013217231\n",
      "[LOG 20180125-09_06_12] training status, epoch: [0010/0010], batch: 1400, loss: 0.0003702825\n",
      "[LOG 20180125-09_06_15] training status, epoch: [0010/0010], batch: 1500, loss: 0.0001820832\n",
      "[LOG 20180125-09_06_18] training status, epoch: [0010/0010], batch: 1600, loss: 0.0003026466\n",
      "[LOG 20180125-09_06_21] training status, epoch: [0010/0010], batch: 1700, loss: 0.0001864629\n",
      "[LOG 20180125-09_06_23] training status, epoch: [0010/0010], batch: 1800, loss: 0.0001655325\n",
      "[LOG 20180125-09_06_26] training status, epoch: [0010/0010], batch: 1900, loss: 0.0000604192\n",
      "[LOG 20180125-09_06_29] training status, epoch: [0010/0010], batch: 2000, loss: 0.0003092249\n",
      "[LOG 20180125-09_06_32] training status, epoch: [0010/0010], batch: 2100, loss: 0.0000612267\n",
      "[LOG 20180125-09_06_36] training status, epoch: [0010/0010], batch: 2200, loss: 0.0001154397\n",
      "[LOG 20180125-09_06_44] training status, epoch: [0010/0010], batch: 2300, loss: 0.0002173854\n",
      "[LOG 20180125-09_06_55] training status, epoch: [0010/0010], batch: 2400, loss: 0.0008612431\n"
     ]
    }
   ],
   "source": [
    "# init deterministic seed\n",
    "seed_value = 1234 #4444 #3333 #2222 #1111 #1234\n",
    "rd.seed(seed_value) # set random seed\n",
    "np.random.seed(seed_value) # set numpy seed\n",
    "torch.manual_seed(seed_value) # set pytorch seed CPU\n",
    "torch.cuda.manual_seed(seed_value) # set pytorch seed GPU\n",
    "\n",
    "# init training parameters\n",
    "num_epochs = 10\n",
    "mini_batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# convert to pytorch tensor - none cuda enabled\n",
    "dataloader = DataLoader(torch_dataset, batch_size=mini_batch_size, shuffle=True, num_workers=0)\n",
    "# set num_workers to zero to retreive deterministic results\n",
    "\n",
    "# determine if CUDA is available at compute node\n",
    "if (torch.backends.cudnn.version() != None):\n",
    "\n",
    "    dataloader = DataLoader(torch_dataset.cuda(), batch_size=mini_batch_size, shuffle=True)\n",
    "\n",
    "# define optimization criterion and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "encoder_optimizer = torch.optim.Adam(encoder_train.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.Adam(decoder_train.parameters(), lr=learning_rate)\n",
    "\n",
    "# train autoencoder model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # init mini batch counter\n",
    "    mini_batch_count = 0\n",
    "\n",
    "    # determine if CUDA is available at compute node\n",
    "    if (torch.backends.cudnn.version() != None) and (use_cuda == True):\n",
    "\n",
    "        # set all networks / models in CPU mode\n",
    "        encoder_train.cuda()\n",
    "        decoder_train.cuda()\n",
    "\n",
    "    # set networks in training mode (apply dropout when needed)\n",
    "    encoder_train.train()\n",
    "    decoder_train.train()\n",
    "\n",
    "    for mini_batch_data in dataloader:\n",
    "\n",
    "        # increase mini batch counter\n",
    "        mini_batch_count += 1\n",
    "\n",
    "        # convert mini batch to torch variable\n",
    "        mini_batch_torch = autograd.Variable(mini_batch_data)\n",
    "\n",
    "        # =================== forward pass =====================\n",
    "\n",
    "        # run forward pass\n",
    "        z_representation = encoder_train(mini_batch_torch) # encode mini-batch data\n",
    "        mini_batch_reconstruction = decoder_train(z_representation) # decode mini-batch data\n",
    "\n",
    "        # determine reconstruction loss\n",
    "        reconstruction_loss = criterion(mini_batch_reconstruction, mini_batch_torch)\n",
    "\n",
    "        # =================== backward pass ====================\n",
    "\n",
    "        # reset graph gradients\n",
    "        decoder_optimizer.zero_grad()\n",
    "        encoder_optimizer.zero_grad()\n",
    "\n",
    "        # run backward pass\n",
    "        reconstruction_loss.backward()\n",
    "\n",
    "        # update network parameters\n",
    "        decoder_optimizer.step()\n",
    "        encoder_optimizer.step()\n",
    "\n",
    "        # =================== log ==============================\n",
    "\n",
    "        if mini_batch_count % 100 == 0:\n",
    "\n",
    "            # print mini batch reconstuction results\n",
    "            now = datetime.utcnow().strftime(\"%Y%m%d-%H_%M_%S\")\n",
    "            print('[LOG {}] training status, epoch: [{:04}/{:04}], batch: {:04}, loss: {:.10f}'.format(now, (epoch+1), num_epochs, mini_batch_count, reconstruction_loss.data[0]))\n",
    "\n",
    "    # save trained encoder model file to disk\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d-%H_%M_%S\")\n",
    "    encoder_model_name = \"{}_ep_{}_encoder_model.pth\".format(now, (epoch+1))\n",
    "    torch.save(encoder_train.state_dict(), os.path.join(\"models\", encoder_model_name))\n",
    "\n",
    "    # save trained decoder model file to disk\n",
    "    decoder_model_name = \"{}_ep_{}_decoder_model.pth\".format(now, (epoch+1))\n",
    "    torch.save(decoder_train.state_dict(), os.path.join(\"models\", decoder_model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06. Result Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# specify trained models to be loaded\n",
    "#encoder_model_name = \"20180125-06_44_14_ep_1_encoder_model.pth\"\n",
    "#decoder_model_name = \"20180125-06_44_14_ep_1_decoder_model.pth\"\n",
    "\n",
    "# init training network classes / architectures\n",
    "encoder_eval = encoder()\n",
    "decoder_eval = decoder()\n",
    "\n",
    "# load trained models\n",
    "encoder_eval.load_state_dict(torch.load(os.path.join(\"models\", encoder_model_name)))\n",
    "decoder_eval.load_state_dict(torch.load(os.path.join(\"models\", decoder_model_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert encoded transactional data to torch Variable\n",
    "data = autograd.Variable(torch_dataset)\n",
    "\n",
    "# set networks in training mode (don't apply dropout)\n",
    "encoder_eval.eval()\n",
    "decoder_eval.eval()\n",
    "\n",
    "# reconstruct encoded transactional data\n",
    "reconstruction = decoder_eval(encoder_eval(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20180125-09:07:16] collected reconstruction loss of: 307457/307457 transactions\n",
      "[LOG 20180125-09:07:16] reconstruction loss: 0.0003191435\n"
     ]
    }
   ],
   "source": [
    "# determine reconstruction loss - all transactions\n",
    "reconstruction_loss_all = criterion(reconstruction, data)\n",
    "\n",
    "# print reconstruction loss - all transactions\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] collected reconstruction loss of: {:06}/{:06} transactions'.format(now, reconstruction.size()[0], reconstruction.size()[0]))\n",
    "print('[LOG {}] reconstruction loss: {:.10f}'.format(now, reconstruction_loss_all.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20180125-09:07:16] collected individial reconstruction loss of: 000000/307457 transactions\n",
      "[LOG 20180125-09:07:28] collected individial reconstruction loss of: 100000/307457 transactions\n",
      "[LOG 20180125-09:07:38] collected individial reconstruction loss of: 200000/307457 transactions\n",
      "[LOG 20180125-09:07:49] collected individial reconstruction loss of: 300000/307457 transactions\n"
     ]
    }
   ],
   "source": [
    "# init binary cross entropy errors\n",
    "reconstruction_loss_transaction = np.zeros(reconstruction.size()[0])\n",
    "\n",
    "# iterate over all detailed reconstructions\n",
    "for i in range(0, reconstruction.size()[0]):\n",
    "\n",
    "    # determine reconstruction loss - individual transactions\n",
    "    reconstruction_loss_transaction[i] = criterion(reconstruction[i], data[i])\n",
    "\n",
    "    if(i % 100000 == 0):\n",
    "\n",
    "        ### print conversion summary\n",
    "        now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "        print('[LOG {}] collected individial reconstruction loss of: {:06}/{:06} transactions'.format(now, i, reconstruction.size()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x109b77b10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108ac1650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "labels = np.array(ori_dataset.Artifical_Outlier)\n",
    "ids = np.array(ori_dataset.ID_Key, ori_dataset.Artifical_Outlier)\n",
    "\n",
    "regular_x = ids[np.where(labels==0)]\n",
    "regular_y = reconstruction_loss_transaction[np.where(labels==0)]\n",
    "\n",
    "# plot error scatter plot\n",
    "ax.scatter(regular_x, regular_y, c='C0', alpha=0.4, marker=\"o\") # plot regular transactions\n",
    "\n",
    "anomaly_1_x = ids[np.where(labels==1)]\n",
    "anomaly_1_y = reconstruction_loss_transaction[np.where(labels==1)]\n",
    "\n",
    "# plot error scatter plot\n",
    "ax.scatter(anomaly_1_x, anomaly_1_y, c='C1', marker=\"^\") # plot regular transactions\n",
    "\n",
    "anomaly_2_x = ids[np.where(labels==2)]\n",
    "anomaly_2_y = reconstruction_loss_transaction[np.where(labels==2)]\n",
    "\n",
    "# plot error scatter plot\n",
    "ax.scatter(anomaly_2_x, anomaly_2_y, c='C3', marker=\"^\") # plot regular transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07. Optional Excercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo - Timur and Marco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08. Lab Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo - Timur and Marco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 09. Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo - Timur and Marco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
