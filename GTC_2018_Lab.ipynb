{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"./images/gtc_logo.png\", class=\"img-thumbnail\" align=\"left\", width=220, height=240>\n",
    "\n",
    "## <center>\"Detection of Anomalies in Financial Transactions <br/> using Deep Autoencoder Networks\" <br/> - Draft Version - </center><br/> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content of this GPU Technology Conference (GTC) 2018 lab was jointly developed by Marco Schreyer and Timur Sattarov. Pls. don't hesitate to contact us in case of any questions via <a href=\"mailto:marco.schreyer@dfki.de\">marco.schreyer@dfki.de</a> and <a href=\"mailto:sattarov.timur@pwc.com\">sattarov.timur@pwc.com</a>.\n",
    "\n",
    "Major elements of the lab content are inspired by the following publication ***\"Detection of Anomalies in Large Scale Accounting Data using Deep Autoencoder Networks\"***, of M. Schreyer, T. Sattarov, D. S. Borth, A. Dengel, and B. Reimer, 2017 (arXiv preprint available under: https://arxiv.org/abs/1709.05254)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction and Lab Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Fraud and Accounting Information Systems (AIS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Association of Certified Fraud Examiners estimates in its Global Fraud Study 2016 [1] that the typical organization loses 5% of its annual revenues due to fraud. According to Joseph T. Wells [2] the term **\"fraud\"** refers to, \n",
    "\n",
    ">_\"the abuse of one's occupation for personal enrichment through the deliberate misuse of an organization's resources or assets\"_. \n",
    "\n",
    "A similar more recent study, conducted by the auditors of PwC, revealed that 30% of the study respondents experienced losses of between \\$100,000 and \\$5 million USD [3] in the last 24 months. The study also showed that financial statement fraud caused by far the greatest median loss of the surveyed fraud schemes.\n",
    "\n",
    "At the same time organizations accelerate the digitization and reconfiguration of business processes [4] affecting in particular Accounting Information Systems (AIS) or more general Enterprise Resource Planning (ERP) systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" style=\"max-width: 550px; height: auto\" src=\"images/accounting.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 1:** Hierarchical view of an Accounting Information System (AIS) that records distinct layer of abstractions, namely (1) the business process information, (2) the accounting information as well as the (3) technical journal entry information in designated database tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steadily, these systems collect vast quantities of electronic evidence at an almost atomic level. This holds in particular for the journal entries of an organization recorded in its general ledger and sub-ledger accounts. SAP, one of the most prominent ERP software providers, estimates that approx. 76% of the world's transaction revenue touches one of their systems [5].\n",
    "\n",
    "The illustration in **Figure 1** depicts a hierarchical view of an Accounting Information System (AIS) recording process and journal entry information in designated database tables. In the context of fraud examinations the data collected by such systems may contain valuable traces of a potential fraud scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Classification of Financial Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When conducting a detailed examination of real-world journal entries, usually recorded in large-scaled AIS ERP systems, two prevalent characteristics can be observed:\n",
    "\n",
    "> - specific transactions attributes exhibit **a high variety of distinct attribute values** e.g. customer information, posted sub-ledgers, amount information, and \n",
    "> - the transactions exhibit **strong dependencies between specific attribute values** e.g. between customer informaiton and type of payment, posting type and general ledgers. \n",
    "\n",
    "Derived from this observation we distinguish two classes of anomalous journal entries, namely **\"global\"** and **\"local\" anomalies** as illustrated in **Figure 2** below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" style=\"max-width: 450px; height: auto\" src=\"images/anomalies.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 2:** Illustrative example of global and local anomalies portrait in a feature space of the two transaction features \"Posting Amount\" (Feature 1) and \"Posting Positions\" (Feature 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Global Anomalies***, are financial transactions that exhibit **unusual or rare individual attribute values**. These anomalies usually relate to highly skewed attributes e.g. seldom posting users, rarely used ledgers, or unusual posting times. \n",
    "\n",
    "Traditionally \"red-flag\" tests, performed by auditors during annual audits, are designed to capture those types of anomalies. However, such tests might result in a high volume of false positive alerts due to e.g. regular reverse postings, provisions and year-end adjustments usually associated with a low fraud risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Local Anomalies***, are financial transactions that exhibit an **unusual or rare combination of attribute values** while the individual attribute values occur quite frequently e.g. unusual accounting records. \n",
    "\n",
    "This type of anomalies is significantly more difficult to detect since perpetrators intend to disguise their activities trying to imitate a regular behavior. As a result, such anomalies usually pose a high fraud risk since they might correspond to e.g. misused user accounts, irregular combinations of general ledger accounts and posting keys that don't follow usual a usual activity pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Lab Objective and Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this lab is to walk you through a deep learning based methodology that can be used to detect of global and local anomalies in financial datasets. The proposed method is based on the following assumptions: \n",
    "\n",
    ">1. the majority of financial transactions recorded within an organizations’ ERP-system relate to regular day-to-day business activities and perpetrators need to deviate from the ”regular” in order to conduct fraud,\n",
    ">2. such deviating behavior will be recorded by a very limited number of financial transactions and their respective attribute values or combination of attribute values and we refer to such deviations as \"anomalies\".\n",
    "\n",
    "Concluding from these assumptions we can learn a model of regular journal entries with minimal ”harm” caused by the potential anomalous ones.\n",
    "\n",
    "In order to detect such anomalies we will train deep autoencoder networks to learn a compressed but \"lossy\" model of regular transactions and their underlying posting pattern. Imposing a strong regularization onto the networks hidden layers limits the networks' ability to memorize the characteristics of anomalous journal entries. Once the training process is completed, the network will be able to reconstruct regular journal entries, while failing to do so for the anomalous ones.\n",
    "\n",
    "After completing the lab you should be familiar with:\n",
    "\n",
    ">1. the basic concepts, intuitions and major building blocks of autoencoder neural networks,\n",
    ">2. the techniques of pre-processing financial data in order to learn a model of its characteristics,\n",
    ">3. the application of autoencoder neural networks to detect anomalies in large-scale financial data, and,\n",
    ">4. the interpretation of the detection results of the networks as well as its reconstruction loss. \n",
    "\n",
    "Please note, that this lab is not a complete nor comprehensive forensic data analysis approach or fraud examination strategy. However, the methodology and code provided in this lab can be repurposed or adopted to detect anomalous records in a variety of financial datasets. Subsequently, the detected records might serve as a starting point for a more detailed and substantive examination by auditors or compliance personel. \n",
    "\n",
    "For this lab we assume that you are familiar with the general concepts of deep neural networks (DNN) and GPUs as well as PyTorch and Python. For more information on these concepts please check the relevant labs of NVIDIA's Deep Learning Institute (DLI). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercises: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about potential fraud scenarios of your organization:\n",
    "\n",
    ">1. What scenarions or fraudulent activities you could think of? [3 min]\n",
    ">2. What data sources might affect or record those potential fraudulent activities? [5 min]\n",
    ">3. What kind of data analytics techniques could be applied to detect those activities? [5 min]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup and Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Python Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, let's verify that Python is working on your system. To do this, execute the cell block below by giving it focus (clicking on it with your mouse), and hitting Shift-Enter, or pressing the play button in the toolbar above. If all goes well, you should see some output returned below the grey cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer should be forty-two: 42\n"
     ]
    }
   ],
   "source": [
    "print('The answer should be forty-two: {}'.format(str(40+2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Python Libraries Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step let's import the libraries needed throughout the lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing utilities\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# importing pytorch libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import autograd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# importing data science libraries\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 CUDNN and GPU Verfication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine if CDNN is available on the server let's execute the cell below to display information about the available CUDNN version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20180306-09:46:47] The CUDNN backend version: None\n"
     ]
    }
   ],
   "source": [
    "# print CUDNN backend version\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] The CUDNN backend version: {}'.format(now, torch.backends.cudnn.version()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's display information about the potential GPUs running on the server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: nvidia-smi: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If CUDNN and GPU's are available let's still specify if we want to use both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Python and PyTorch Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's execute the cell below to display information about the Python and PyTorch version running on the server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20180306-09:46:52] The Python version: 2.7.14 (default, Feb  1 2018, 16:41:55) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.39.2)]\n"
     ]
    }
   ],
   "source": [
    "# print current PyTorch version\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] The Python version: {}'.format(now, sys.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20180306-09:46:53] The PyTorch version: 0.3.0.post4\n"
     ]
    }
   ],
   "source": [
    "# print current PyTorch version\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] The PyTorch version: {}'.format(now, torch.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Random Seed Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let' set the seeds of random elements in the code e.g. the initilization of the network paramaters to guarantee deterministic computation and results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init deterministic seed\n",
    "seed_value = 1234 #4444 #3333 #2222 #1111 #1234\n",
    "rd.seed(seed_value) # set random seed\n",
    "np.random.seed(seed_value) # set numpy seed\n",
    "torch.manual_seed(seed_value) # set pytorch seed CPU\n",
    "if (torch.backends.cudnn.version() != None and USE_CUDA == True):\n",
    "    torch.cuda.manual_seed(seed_value) # set pytorch seed GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Financial Fraud Detection Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will conduct a descriptive analysis of the labs financial dataset. Furthermore, we will apply some necessary pre-processing steps to train a deep neural network. The lab is based on a derivative of the **\"Synthetic Financial Dataset For Fraud Detection\"** by Lopez-Rojas [6] available via the Kaggle predictive modelling and analytics competitions platform that can be obtained using the following link: https://www.kaggle.com/ntnu-testimon/paysim1.\n",
    "\n",
    "Let's start loading the dataset and investigate its structure and attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset the dataset into the notebook kernel\n",
    "ori_dataset = pd.read_csv('./data/fraud_dataset_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(533009, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect the datasets dimensionalities\n",
    "ori_dataset.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Initial Data and Attribute Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We augmented the dataset and renamed a the columns to appear more similar to a real-world dataset that one usually observes in SAP-ERP systems as part of SAP's Finance and Cost controlling (FICO) module. \n",
    "\n",
    "The dataset contains a subset of in total 6 categorical and 2 numerical attributes available in the FICO BKPF (containing the posted journal entry headers) and BSEG (containing the posted journal entry segments) tables. Please, find below a list of the individual features as well as a brief description of their respective semantics:\n",
    "\n",
    ">- `BELNR`: The accounting document number,\n",
    ">- `BUKRS`: The company code,\n",
    ">- `BSCHL`: The posting key,\n",
    ">- `HKONT`: The posted general ledger Account,\n",
    ">- `WAERS`: The currency key,\n",
    ">- `KTOSL`: The general ledger account key,\n",
    ">- `DMBTR`: The amount in local currency,\n",
    ">- `WRBTR`: The amount in document currency.\n",
    "\n",
    "Let's also have a closer look into the top 10 rows of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WAERS</th>\n",
       "      <th>BUKRS</th>\n",
       "      <th>KTOSL</th>\n",
       "      <th>BELNR</th>\n",
       "      <th>BSCHL</th>\n",
       "      <th>HKONT</th>\n",
       "      <th>DMBTR</th>\n",
       "      <th>WRBTR</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C3</td>\n",
       "      <td>C31</td>\n",
       "      <td>C9</td>\n",
       "      <td>C92</td>\n",
       "      <td>A3</td>\n",
       "      <td>B1</td>\n",
       "      <td>280979.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C1</td>\n",
       "      <td>C18</td>\n",
       "      <td>C7</td>\n",
       "      <td>C76</td>\n",
       "      <td>A1</td>\n",
       "      <td>B2</td>\n",
       "      <td>129856.53</td>\n",
       "      <td>243343.00</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C1</td>\n",
       "      <td>C19</td>\n",
       "      <td>C2</td>\n",
       "      <td>C20</td>\n",
       "      <td>A1</td>\n",
       "      <td>B3</td>\n",
       "      <td>957463.97</td>\n",
       "      <td>3183838.41</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C4</td>\n",
       "      <td>C48</td>\n",
       "      <td>C9</td>\n",
       "      <td>C95</td>\n",
       "      <td>A2</td>\n",
       "      <td>B1</td>\n",
       "      <td>2681709.51</td>\n",
       "      <td>28778.00</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C5</td>\n",
       "      <td>C58</td>\n",
       "      <td>C1</td>\n",
       "      <td>C19</td>\n",
       "      <td>A3</td>\n",
       "      <td>B1</td>\n",
       "      <td>910514.49</td>\n",
       "      <td>346.00</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C1</td>\n",
       "      <td>C15</td>\n",
       "      <td>C6</td>\n",
       "      <td>C68</td>\n",
       "      <td>A1</td>\n",
       "      <td>B2</td>\n",
       "      <td>357627.56</td>\n",
       "      <td>704520.00</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C4</td>\n",
       "      <td>C47</td>\n",
       "      <td>C2</td>\n",
       "      <td>C28</td>\n",
       "      <td>A2</td>\n",
       "      <td>B3</td>\n",
       "      <td>955576.84</td>\n",
       "      <td>128328.00</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C1</td>\n",
       "      <td>C19</td>\n",
       "      <td>C1</td>\n",
       "      <td>C17</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>41769.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C4</td>\n",
       "      <td>C40</td>\n",
       "      <td>C9</td>\n",
       "      <td>C97</td>\n",
       "      <td>A2</td>\n",
       "      <td>B1</td>\n",
       "      <td>44309.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>C6</td>\n",
       "      <td>C69</td>\n",
       "      <td>C1</td>\n",
       "      <td>C12</td>\n",
       "      <td>A2</td>\n",
       "      <td>B1</td>\n",
       "      <td>466720.45</td>\n",
       "      <td>43843.00</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  WAERS BUKRS KTOSL BELNR BSCHL HKONT       DMBTR       WRBTR    label\n",
       "0    C3   C31    C9   C92    A3    B1   280979.60        0.00  regular\n",
       "1    C1   C18    C7   C76    A1    B2   129856.53   243343.00  regular\n",
       "2    C1   C19    C2   C20    A1    B3   957463.97  3183838.41  regular\n",
       "3    C4   C48    C9   C95    A2    B1  2681709.51    28778.00  regular\n",
       "4    C5   C58    C1   C19    A3    B1   910514.49      346.00  regular\n",
       "5    C1   C15    C6   C68    A1    B2   357627.56   704520.00  regular\n",
       "6    C4   C47    C2   C28    A2    B3   955576.84   128328.00  regular\n",
       "7    C1   C19    C1   C17    A1    B1    41769.26        0.00  regular\n",
       "8    C4   C40    C9   C97    A2    B1    44309.79        0.00  regular\n",
       "9    C6   C69    C1   C12    A2    B1   466720.45    43843.00  regular"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect top rows of dataset\n",
    "ori_dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also have noticed the attribute `label` in the data. We will use this field througout the lab to evaluate quality of our trained models. The field describes the true nature of each individual transaction either beeing a **regular** transaction (denoted by `regular`) or an **anomaly** (denoted by `global`and `local`). Let's have closer look into the distribution of the regular vs. anomalous transactions in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "regular    532909\n",
       "global         70\n",
       "local          30\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of anomalies vs. regular transactions\n",
    "ori_dataset.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the statistic reveals that, similiar to real world scenarios, we are facing a highly \"unbalanced\" dataset. Overall, the dataset contains only a small fraction of **100 (0.018%)** anomalous transactions. While the 100 anomalous entries encompass **70 (0.013%)** \"global\" anomalies and **30 (0.005%)** \"local\" anomalies as introduced in section 1.2 of the lab notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the label aside\n",
    "label = ori_dataset.pop('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Pre-Processing of Categorical Transaction Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the initial data assessment above we can observe that the majority of attributes recorded in AIS- and ERP-systems correspond to categorical (discrete) attribute values, e.g. the posting date, the general-ledger account, the posting type, the currency. Let's have a more detailed look into the distribution of dataset two attributes (1) the posting key as well as (2) the general ledger account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10b379890>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a866e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare to plot posting key and general ledger account side by side\n",
    "fig, ax =plt.subplots(1,2)\n",
    "fig.set_figwidth(20)\n",
    "\n",
    "# plot distribution of the posting key attribute\n",
    "g = sns.countplot(x=ori_dataset['BSCHL'], ax=ax[0])\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=90)\n",
    "\n",
    "# plot distribution of the general ledger account attribute\n",
    "sns.countplot(x=ori_dataset['HKONT'], ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, neural networks are in general not designed to be trained directly on categorical data and require the attributes to be trained on to be numeric. One simple way the meet this requirement is by applying a technique referred to as **\"one-hot\" encoding**. Using this encoding technique we will derive a numerical representation of each of the categorical attribute values. One-hot encoding creates a new binary columns for each categorical attribute value that indicates the presence of the value in the original data. \n",
    "\n",
    "Let's work through a brief example: The **categorical attribute “Receiver”** below contains the names \"John\", \"Timur\" and \"Marco\". We \"one-hot\" encode the names by creating a separate binary column for each possible value. Wherever the original value was \"John\", we save 1.0 in the created \"John\" column and save 0.0 values for all the other names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" style=\"max-width: 430px; height: auto\" src=\"images/encoding.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this technique will one-hot encode the 6 categorical attributes in the original dataset. This can be easily achived by using the `get_dummies()` function already available in the Pandas data science library:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select categorical attributes to be \"one-hot\" encoded\n",
    "categorical_attr_names = ['KTOSL', 'BELNR', 'BSCHL', 'HKONT']\n",
    "\n",
    "# encode categorical attributes into a binary one-hot encoced representation \n",
    "ori_dataset_categ_transformed = pd.get_dummies(ori_dataset[categorical_attr_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the encoding of 10 sample transactions to see if we have been successfull:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect encoded sample transactions\n",
    "ori_dataset_categ_transformed.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Pre-Processing of Numerical Transaction Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now inspect the distributions of the two numerical attributes contained in the dataset namely, (1) local currency amount and (2) document currency amount:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot \"DMBTR\" attribute and its log scale\n",
    "fig, ax =plt.subplots(1,2)\n",
    "fig.set_figwidth(15)\n",
    "sns.distplot(ori_dataset['DMBTR'], ax=ax[0])\n",
    "sns.distplot(ori_dataset['WRBTR'], ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, it can be observed, that for both attributes the distributions of amount values are heavy tailed. In order to faster approach a potential global minima scaling and normalization of numerical input values is good practice. Therefore, we first log-scale both variables and second min-max normalize to the scaled amounts to the intervall [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select \"DMBTR\" vs. \"WRBTR\" attribute\n",
    "numeric_attr_names = ['DMBTR', 'WRBTR']\n",
    "\n",
    "# add a small epsilon to eliminate zero values from data for log scaling\n",
    "numeric_attr = ori_dataset[numeric_attr_names] + 1e-7\n",
    "numeric_attr = numeric_attr.apply(np.log)\n",
    "\n",
    "# normalize all numeric attribute to the range [0,1]\n",
    "ori_dataset_numeric_attr = (numeric_attr - numeric_attr.min()) / (numeric_attr.max() - numeric_attr.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize the log-scaled and min-max normalized distributions of both attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append 'label' attribute for color distinction\n",
    "numeric_attr_vis = ori_dataset_numeric_attr.copy()\n",
    "numeric_attr_vis['label'] = label\n",
    "\n",
    "# plot numeric attributes scaled under natural log\n",
    "g = sns.pairplot(data=numeric_attr_vis, vars=numeric_attr_names, hue='label')\n",
    "g.fig.set_size_inches(15, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, as anticipated the \"global\" anomalies (green) due to their unusual amount values fall outside the range of the main amount distributions. In contrast the \"local\" anomalies (orange) are much more commingled in the regular transaction amounts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Merge Categorical and Numerical Transaction Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we merge both pre-processed numerical and categorical column-wise into a single dataset that we will use for training our deep autoencoder neural network (explained in the following section 4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge categorical and numeric subsets\n",
    "ori_subset_transformed = pd.concat([ori_dataset_categ_transformed, ori_dataset_numeric_attr], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's again have a look at the dimensionality of the dataset after we applied the distinct preprocessing steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect final dimensions of pre-processed transactional data\n",
    "ori_subset_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercises: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the preprocessing steps above you may have noticed, that we didn't encode the attributes `WAERS` and `BUKRS` yet. This we left as an excercise for you:\n",
    "\n",
    ">1. Plot and inspect the distribution of the values for both attributes `WAERS` and `BUKRS`. [3 min]\n",
    ">2. Encode both variables using the `get_dummies()` method provided by the Pandas library. [5 min]\n",
    ">3. Merge your encoding results with the Pandas `ori_subset_transformed` data frame. [5 min]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, upon completion of all the pre-processing steps (incl. the excercises) we should end up with a total number of **618 encoded attributes** for each individual transaction. Let's keep this number in mind since it defines the dimensionality of the input- and output-layer of our deep autoencoder network which we will implement in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Autoencoder Neural Networks (AENNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this section is to familiarize ourselves with the underlying idea and concepts of building a deep autoencoder neural network. We will cover the major building blocks and the specifc network structure of AENNs as well as an exemplary implementation using the open source machine learning libary PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Autoencoder Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AENNs or replicator neural network are a variant of general feed-forward neural networks that have been initally introduced by Hinton and Salakhutdinov in [6]. AENNs usually comprise a **symmetrical network architecture** as well as a the central hidden layer, referred to as **\"latent\"** or **\"coding\" layer**, of lower dimensionality. The design is chosen intentionally since the training objective of an AENN is to reconstruct its input in a \"self-supervised\" manner. \n",
    "\n",
    "**Figure 3** below illustrates a schematic view of an autoencoder neural network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" style=\"max-width: 600px; height: auto\" src=\"images/autoencoder.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 3:** Schematic view of an autoencoder network comprised of two non-linear mappings (fully connected feed forward neural networks) referred to as encoder $f_\\theta: \\mathbb{R}^{dx} \\mapsto \\mathbb{R}^{dz}$ and decoder $g_\\theta: \\mathbb{R}^{dz} \\mapsto \\mathbb{R}^{dy}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, AAENs can be interepreted as \"lossy\" data **compression algorithms**. They are \"lossy\" in a sense that the reconstructed outputs will be degraded compared to the original inputs. The difference between the original input $x^i$ and its reconstruction $\\hat{x}^i$is referred to as **reconstruction error**. In general AENN encompass three major building blocks:\n",
    "\n",
    "\n",
    ">   1. an encoding mapping function $f_\\theta$, \n",
    ">   2. a decoding mapping function $g_\\theta$, \n",
    ">   3. and a loss function $\\mathcal{L_{\\theta}}$.\n",
    "\n",
    "Most commonly the encoder and the decoder mapping functions consist of **several layers of neurons followed by a nonlinear function** and shared parameters $\\theta$. The encoder mapping $f_\\theta(\\cdot)$ maps an input vector $x^i$ to compressed representation $z^i$ referred to as latent space $Z$. This hidden representation $z^i$ is then mapped back by the decoder $g_\\theta(\\cdot)$ to a re-constructed vector $x^i$ of the original input space. Formally, the nonlinear mappings of the encoder and the decoder can be defined by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>$f_\\theta(x^i) = s(Wx^i + b)$, and $g_\\theta(z^i) = s′(W′z^i + d)$,</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $s$ and $s′$ denote a non-linear activations with model parameters $\\theta = \\{W, b, W', d\\}$, $W \\in \\mathbb{R}^{d_x \\times d_z}, W' \\in \\mathbb{R}^{d_z \\times d_y}$ are weight matrices and $b \\in \\mathbb{R}^{dx}$, $d \\in \\mathbb{R}^{dz}$ are the offset bias vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Autoencoder Neural Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's start implementing an AENN by first implementing the encoder network using PyTorch. For the encoder we aim to implement a model consisting of **eight fully-connected layers**. The model is specified by the following number of neurons per layer: \"618-256-128-64-32-16-8-3\". Meaning the first layer consists of 618 neurons (specified by the dimensionality of our input data), the second layer of 256 neurons and the subsequent layers of 128, 64, 32, 16, 8 and 3 neurons respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some elements of the encoder network code below should be given particular attention:\n",
    "\n",
    ">- `self.encoder_Lx`: defines the linear transformation of the layer applied to the incoming data: $Wx + b$.\n",
    ">- `nn.init.xavier_uniform`: inits the layer weights using a uniform distribution according to [6]. \n",
    ">- `self.encoder_Rx`: defines the non-linear transformation of the layer: $\\sigma(\\cdot)$.\n",
    ">- `self.dropout`: randomly zeroes some of the elements of the input tensor with probability $p$ according to [9].\n",
    "\n",
    "We use **\"Leaky ReLUs\"** as introduced by Xu et al. in [7] to avoid \"dying\" non-linearities and to speed up training convergence. Leaky ReLUs allow a small gradient even when a particular neuron is not active. Finally, we set the **\"drop-out\" probability** to $p=0.2$ (20%), as introduced by [8] in, for each neuron to be set to zero at a forward pass to prevent the network from overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of the encoder network\n",
    "class encoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(encoder, self).__init__()\n",
    "\n",
    "        # specify layer 1 - in 618, out 512\n",
    "        self.encoder_L1 = nn.Linear(in_features=618, out_features=512, bias=True) # add linearity \n",
    "        nn.init.xavier_uniform(self.encoder_L1.weight) # init weights according to [6]\n",
    "        self.encoder_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [todo]\n",
    "\n",
    "        # specify layer 2 - in 512, out 256\n",
    "        self.encoder_L2 = nn.Linear(512, 256, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L2.weight)\n",
    "        self.encoder_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 3 - in 256, out 128\n",
    "        self.encoder_L3 = nn.Linear(256, 128, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L3.weight)\n",
    "        self.encoder_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 4 - in 128, out 64\n",
    "        self.encoder_L4 = nn.Linear(128, 64, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L4.weight)\n",
    "        self.encoder_R4 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 5 - in 64, out 32\n",
    "        self.encoder_L5 = nn.Linear(64, 32, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L5.weight)\n",
    "        self.encoder_R5 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 6 - in 32, out 16\n",
    "        self.encoder_L6 = nn.Linear(32, 16, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L6.weight)\n",
    "        self.encoder_R6 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 7 - in 16, out 8\n",
    "        self.encoder_L7 = nn.Linear(16, 8, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L7.weight)\n",
    "        self.encoder_R7 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 8 - in 8, out 4\n",
    "        self.encoder_L8 = nn.Linear(8, 4, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L8.weight)\n",
    "        self.encoder_R8 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 9 - in 4, out 3\n",
    "        self.encoder_L9 = nn.Linear(4, 3, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L9.weight)\n",
    "        self.encoder_R9 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # init dropout layer with probability p\n",
    "        self.dropout = nn.Dropout(p=0.0, inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # define forward pass through the network\n",
    "        x = self.encoder_R1(self.dropout(self.encoder_L1(x)))\n",
    "        x = self.encoder_R2(self.dropout(self.encoder_L2(x)))\n",
    "        x = self.encoder_R3(self.dropout(self.encoder_L3(x)))\n",
    "        x = self.encoder_R4(self.dropout(self.encoder_L4(x)))\n",
    "        x = self.encoder_R5(self.dropout(self.encoder_L5(x)))\n",
    "        x = self.encoder_R6(self.dropout(self.encoder_L6(x)))\n",
    "        x = self.encoder_R7(self.dropout(self.encoder_L7(x)))\n",
    "        x = self.encoder_R8(self.dropout(self.encoder_L8(x)))\n",
    "        x = self.encoder_R9(self.encoder_L9(x)) # don't apply dropout to the AE bottleneck\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to intstantiate the encoder model for CPU tensors or using CUDNN for CUDA tensor types (to utilize potential available GPUs for computation) by execution of the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init training network classes / architectures\n",
    "encoder_train = encoder()\n",
    "\n",
    "# push to cuda if cudnn is available\n",
    "if (torch.backends.cudnn.version() != None and USE_CUDA == True):\n",
    "    encoder_train = encoder().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is initialized we can visualize the model structure and review the implemented network architecture by execution of the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the initialized architectures\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] encoder architecture:\\n\\n{}\\n'.format(now, encoder_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks, great? Excellent!\n",
    "\n",
    "Let's, now as a next step, complete the AENN implementation by implementing the corresponding decoder network. The design of the decoder model architecture also consists of eight fully-connected layers. The decoder model is intended to **symmetrically mirror** the encoder architecture by a layerwise inversion \"8-16-32-64-128-256-618\" of the encoder network layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of the decoder network\n",
    "class decoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(decoder, self).__init__()\n",
    "\n",
    "        # specify layer 1 - in 3, out 4\n",
    "        self.decoder_L1 = nn.Linear(in_features=3, out_features=4, bias=True) # add linearity \n",
    "        nn.init.xavier_uniform(self.decoder_L1.weight)  # init weights according to [todo]\n",
    "        self.decoder_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [todo]\n",
    "\n",
    "        # specify layer 2 - in 4, out 8\n",
    "        self.decoder_L2 = nn.Linear(4, 8, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L2.weight)\n",
    "        self.decoder_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 3 - in 8, out 16\n",
    "        self.decoder_L3 = nn.Linear(8, 16, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L3.weight)\n",
    "        self.decoder_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 4 - in 16, out 32\n",
    "        self.decoder_L4 = nn.Linear(16, 32, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L4.weight)\n",
    "        self.decoder_R4 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 5 - in 32, out 64\n",
    "        self.decoder_L5 = nn.Linear(32, 64, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L5.weight)\n",
    "        self.decoder_R5 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 6 - in 64, out 128\n",
    "        self.decoder_L6 = nn.Linear(64, 128, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L6.weight)\n",
    "        self.decoder_R6 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "        \n",
    "        # specify layer 7 - in 128, out 256\n",
    "        self.decoder_L7 = nn.Linear(128, 256, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L7.weight)\n",
    "        self.decoder_R7 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 8 - in 256, out 512\n",
    "        self.decoder_L8 = nn.Linear(256, 512, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L8.weight)\n",
    "        self.decoder_R8 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 9 - in 512, out 618\n",
    "        self.decoder_L9 = nn.Linear(512, 618, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L9.weight)\n",
    "        self.decoder_R9 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # init dropout layer with probability p\n",
    "        self.dropout = nn.Dropout(p=0.0, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # define forward pass through the network\n",
    "        x = self.decoder_R1(self.dropout(self.decoder_L1(x)))\n",
    "        x = self.decoder_R2(self.dropout(self.decoder_L2(x)))\n",
    "        x = self.decoder_R3(self.dropout(self.decoder_L3(x)))\n",
    "        x = self.decoder_R4(self.dropout(self.decoder_L4(x)))\n",
    "        x = self.decoder_R5(self.dropout(self.decoder_L5(x)))\n",
    "        x = self.decoder_R6(self.dropout(self.decoder_L6(x)))\n",
    "        x = self.decoder_R7(self.dropout(self.decoder_L7(x)))\n",
    "        x = self.decoder_R8(self.dropout(self.decoder_L8(x)))\n",
    "        x = self.decoder_R9(self.decoder_L9(x)) # don't apply dropout to the AE output\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also intstantiate the encoder model for CPU tensors or using CUDNN for CUDA tensor types (to utilize potential available GPUs for computation) and convince ourselves that it was successfully initialized by printing and reviewing the initialized architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init training network classes / architectures\n",
    "decoder_train = decoder()\n",
    "\n",
    "# push to cuda if cudnn is available\n",
    "if (torch.backends.cudnn.version() != None):\n",
    "    decoder_train = decoder().cuda()\n",
    "    \n",
    "# print the initialized architectures\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] decoder architecture:\\n\\n{}\\n'.format(now, decoder_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like intended? Brilliant!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Autoencoder Neural Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have implemented the AENN and are ready to go, we need to define a loss function suitable to train it. Remember, we aim to train our model to learn a set of encoder-decoder model parameters $\\theta$ that minimize the dissimilarity of a given financial transaction $x^{i}$ and its reconstruction $\\hat{x}^{i} = g_\\theta(f_\\theta(x^{i}))$ as faithfully as possible. \n",
    "\n",
    "Thereby, the training objective is to learn a set of optimal shared encoder-decoder model parameters $\\theta^*$ that optimizes $\\arg\\min_{\\theta} \\|X - g_\\theta(f_\\theta(X))\\|$ over all journal entries $X$. To achieve this optimization objective one typically minimizes a loss function $\\mathcal{L_{\\theta}}$ as part of the network training. In this lab we use the **binary-cross-entropy error (BCE)** loss, defined by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> $\\mathcal{L^{BCE}_{\\theta}}(x^{i};\\hat{x}^{i}) = \\frac{1}{n}\\sum_{i=1}^{n}\\sum_{j=1}^{k} x^{i}_{j} ln(\\hat{x}^{i}_{j}) + (1-x^{i}_{j}) ln(1-\\hat{x}^{i}_{j})$, </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for a set of $n$-journal entries $x^{i}$, $i=1,...,n$ and their respective reconstructions $\\hat{x}^{i}$ over all journal entry attributes $j=1,...,k$. The BCE loss will penalize models that result in a high dissimilarity between input transactions and their respective reconstructions. \n",
    "\n",
    "Luckily, an implementation of the BCE loss is already available in PyTorch! It can be instantiated \"off-the-shelf\" via execution of the following PyTorch command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimization criterion / loss function\n",
    "loss_function = nn.BCEWithLogitsLoss(size_average=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: Enabling the parameter `size_average` specifies that the losses are averaged over all observations for each minibatch).\n",
    "\n",
    "Based on the loss magnitude of a certain mini-batch PyTorch automatically computes the gradients. But even better, based on the gradient, PyTorch also helps us in updating the network paramaters $\\theta$. Therefore, several parameter update strategies are already available and can be used. \n",
    "\n",
    "We will use the **Adam optimization** as propsed in [8] and set the learning-rate $l = 0.003$. The optimizer will tweak the model parameter values according to the gradients to minimize the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define learning rate and optimization strategy\n",
    "learning_rate = 1e-3\n",
    "encoder_optimizer = torch.optim.Adam(encoder_train.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.Adam(decoder_train.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have successfully implemented and defined the three AENN building blocks let's take some time to review the `encoder` and `decoder` model definition as well as the `loss`. Carefully read the above code and comments carefully and don't hesitate to let us know any questions you might have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the Autoencoder Neural Network (AAEN) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will train our deep autoencoder neural network (section 4 of the lab) using the pre-processed transactional data (section 3 of the lab). More specifically, we will have a detailed look into the distinct training steps as well as how to monitor the training progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Preparing the Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have specified the AENN model and preprocessed the dataset, let's now start to train our model. Therefore, we train the model for **5 epochs** with a **mini-batch size of 128** journal entries per batch. This implies that the whole dataset will be fed to the AENN 5 times in chunks of 128 journal entries yielding to 4.165 mini-batches per epoch (533.009 journal entries / 128 journal entries per mini-batch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify training parameters\n",
    "num_epochs = 5\n",
    "mini_batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training phase, we will fetch the individual mini-batches of the entire population of journal entries. To achieve this we will use PyTorchs `DataLoader` that provides single- or multi-process iterators over a given dataset to load one mini-batches at a time. By enabling `shuffle=True` the data will be reshuffled at every epoch prior to feeding it to the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pre-processed data to pytorch tensor\n",
    "torch_dataset = torch.from_numpy(ori_subset_transformed.values).float()\n",
    "\n",
    "# convert to pytorch tensor - none cuda enabled\n",
    "dataloader = DataLoader(torch_dataset, batch_size=mini_batch_size, shuffle=True, num_workers=0)\n",
    "# note: we set num_workers to zero to retreive deterministic results\n",
    "\n",
    "# determine if CUDA is available at compute node\n",
    "if (torch.backends.cudnn.version() != None):\n",
    "    dataloader = DataLoader(torch_dataset.cuda(), batch_size=mini_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Running the Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we start start training the model. The training procedure of each minibatch is performed as follows: \n",
    "\n",
    ">1. do a forward pass through the encoder-decoder part, \n",
    ">2. compute the binary-cross-entropy reconstruction loss $\\mathcal{L^{BCE}_{\\theta}}(x^{i};\\hat{x}^{i})$, \n",
    ">3. do a backward pass through the encoder-decoder part, and \n",
    ">4. update the parameteres of the encoder $f_\\theta(\\cdot)$ and decoder $g_\\theta(\\cdot)$ networks.\n",
    "\n",
    "To ensure learning while training our AENN model we will monitor whether the loss decreases with progressing training. Therefore, we obtain and evaluate the reconstruction performance of the entire dataset after each training epoch. Based on this evaluation we can conclude on the training progress and whether the loss is converging which implies that the model is not improving any further.\n",
    "\n",
    "The following elements of the network training code below should be given particular attention:\n",
    " \n",
    ">- `reconstruction_loss.backward()` computes gradients based on the magnitude of the reconstruct loss,\n",
    ">- `decoder_optimizer.step()` and `decoder_optimizer.step()` updates the network parameters based on the gradient.\n",
    "\n",
    "Please also note, that the mini-batch training of the AENN will be executed on the GPU (if CUDNN is available and set accordingly by USE_CUDA=True). However, the evaluation of the reconstruction performance over the entire set of journal entries will be performed at CPU level. Using PyTorch this can be easily achieved with the following commands:\n",
    "\n",
    ">- `encoder_train.cuda()`: moves all model parameters and buffers to the GPU.\n",
    ">- `encoder_train.cpu()`: moves all model parameters and buffers to the CPU.\n",
    "\n",
    "The reason for the switch to the CPU in the evaluation phase is the size of the entire dataset. We aim to compute the reconstruction error over the entire dataset, which in most of the cases does not fit into the memory of GPU.\n",
    "\n",
    "In addition, after each training epoch we want to save a checkpoints of both the `encoder` and `decoder` model. The saved model checkpoints contain a snapshot of the trained model parameter values upon completion of a training epoch. In general it is good practice, to save checkpoints at regular intervals during training. In case your system crashes during training you are able continue from the last checkpoint rather than start over from scratch.\n",
    "\n",
    ">- `torch.save()`: saves a checkpoint of the actual encoder and decoder model parameter values to disc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init collection of mini-batch losses\n",
    "losses = []\n",
    "\n",
    "# convert encoded transactional data to torch Variable\n",
    "data = autograd.Variable(torch_dataset)\n",
    "\n",
    "# train autoencoder model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # init mini batch counter\n",
    "    mini_batch_count = 0\n",
    "\n",
    "    # set networks in training mode (apply dropout when needed)\n",
    "    if (torch.backends.cudnn.version() != None and USE_CUDA == True):\n",
    "        encoder_train.cuda().train()\n",
    "        decoder_train.cuda().train()\n",
    "    else:\n",
    "        encoder_train.cpu().train()\n",
    "        decoder_train.cpu().train()\n",
    "\n",
    "    # iterate over all mini-batches\n",
    "    for mini_batch_data in dataloader:\n",
    "\n",
    "        # increase mini batch counter\n",
    "        mini_batch_count += 1\n",
    "\n",
    "        # convert mini batch to torch variable\n",
    "        mini_batch_torch = autograd.Variable(mini_batch_data)\n",
    "\n",
    "        # =================== (1) forward pass ===================================\n",
    "\n",
    "        # run forward pass\n",
    "        z_representation = encoder_train(mini_batch_torch) # encode mini-batch data\n",
    "        mini_batch_reconstruction = decoder_train(z_representation) # decode mini-batch data\n",
    "        \n",
    "        # =================== (2) compute reconstruction loss ====================\n",
    "\n",
    "        # determine reconstruction loss\n",
    "        reconstruction_loss = loss_function(mini_batch_reconstruction, mini_batch_torch)\n",
    "        \n",
    "        # =================== (3) backward pass ==================================\n",
    "\n",
    "        # reset graph gradients\n",
    "        decoder_optimizer.zero_grad()\n",
    "        encoder_optimizer.zero_grad()\n",
    "\n",
    "        # run backward pass\n",
    "        reconstruction_loss.backward()\n",
    "        \n",
    "        # =================== (4) update model parameters ========================\n",
    "\n",
    "        # update network parameters\n",
    "        decoder_optimizer.step()\n",
    "        encoder_optimizer.step()\n",
    "\n",
    "        # =================== monitor training progress ==========================\n",
    "\n",
    "        # print training progress each 1'000 mini-batches\n",
    "        if mini_batch_count % 1000 == 0:\n",
    "\n",
    "            # print mini batch reconstuction results\n",
    "            now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "            print('[LOG {}] training status, epoch: [{:04}/{:04}], batch: {:04}'.format(now, (epoch+1), num_epochs, mini_batch_count))\n",
    "    \n",
    "    # =================== evaluate model performance =============================\n",
    "    \n",
    "    # set networks in training mode (don't apply dropout)\n",
    "    # move the entire model computation on CPU\n",
    "    encoder_train.cpu().eval()\n",
    "    decoder_train.cpu().eval()\n",
    "\n",
    "    # reconstruct encoded transactional data\n",
    "    reconstruction = decoder_train(encoder_train(data))\n",
    "    \n",
    "    # determine reconstruction loss - all transactions\n",
    "    reconstruction_loss_all = loss_function(reconstruction, data)\n",
    "            \n",
    "    # collect reconstruction loss\n",
    "    losses.extend([reconstruction_loss.data[0]])\n",
    "    \n",
    "    # print reconstuction loss results\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "    print('[LOG {}] training status, epoch: [{:04}/{:04}], loss: {:.10f}'.format(now, (epoch+1), num_epochs, reconstruction_loss.data[0]))\n",
    "\n",
    "    # =================== save model snapshot to disk ============================\n",
    "    \n",
    "    # save trained encoder model file to disk\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d-%H_%M_%S\")\n",
    "    encoder_model_name = \"{}_ep_{}_encoder_model.pth\".format(now, (epoch+1))\n",
    "    torch.save(encoder_train.state_dict(), os.path.join(\"./models\", encoder_model_name))\n",
    "\n",
    "    # save trained decoder model file to disk\n",
    "    decoder_model_name = \"{}_ep_{}_decoder_model.pth\".format(now, (epoch+1))\n",
    "    torch.save(decoder_train.state_dict(), os.path.join(\"./models\", decoder_model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's evaluate if the loss function is indeed going down with progressing training of the model. Therefore, let's visualize the loss history and see how the training progressed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training progress\n",
    "plt.plot(range(0, len(losses)), losses)\n",
    "plt.xlabel('[training epoch]')\n",
    "plt.xlim([0, len(losses)])\n",
    "plt.ylabel('[reconstruction-error]')\n",
    "#plt.ylim([0.0, 1.0])\n",
    "plt.title('AAEN training performance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the reconstruction loss changes as we progressing in training our model? After 5 epochs we can observe that our training loss significantly went down which indicates our network did a pretty good job in learning. Also, the reconstruction error on our training data seem to start converging nicely.\n",
    "\n",
    "But, from the plot we may also observe that the model could probably be trained a few more epochs as the trend for the reconstruction loss decreases for the last few epochs. In order to save time, we will continue the lab using a model that was already pre-trained by 20 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, before we continue with the next notebook section it's time for some exercises:\n",
    "\n",
    ">1. Set the `USE_CUDA` flag to `False` and re-start the training procdedure. What impact do you observe in terms of training time needed for training a single epoch? [5-10 min]\n",
    ">2. Set the `dropout` probability to `0.8` (80%) and re-start the training procedure. What impact do you observe in terms of training performance / reconstruction loss? [5-10 min]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating the Autoencoder Neural Network (AAEN) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to train our autoencoder model, we will explore how we can use such a model to detect anomalies within the entire population of journal entries. Initially, we will start by loading a pre-trained model of 20 epochs and assess its reconstruction capability on the entire dataset. \n",
    "\n",
    "The pre-trained model is stored in the same directory as the lab notebook and can be loaded by executing the cell below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-245aaf0d9e0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# init training network classes / architectures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mencoder_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdecoder_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encoder' is not defined"
     ]
    }
   ],
   "source": [
    "# restore pretrained model checkpoint\n",
    "encoder_model_name = \"20180223-13_18_39_ep_10_encoder_model.pth\"\n",
    "decoder_model_name = \"20180223-13_18_39_ep_10_decoder_model.pth\"\n",
    "\n",
    "# init training network classes / architectures\n",
    "encoder_eval = encoder()\n",
    "decoder_eval = decoder()\n",
    "\n",
    "# load trained models\n",
    "encoder_eval.load_state_dict(torch.load(os.path.join(\"models\", encoder_model_name)))\n",
    "decoder_eval.load_state_dict(torch.load(os.path.join(\"models\", decoder_model_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Assessment of the Pre-Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once obtained, let's use the model to reconstruct the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert encoded transactional data to torch Variable\n",
    "data = autograd.Variable(torch_dataset)\n",
    "\n",
    "# set networks in training mode (don't apply dropout)\n",
    "encoder_eval.eval()\n",
    "decoder_eval.eval()\n",
    "\n",
    "# reconstruct encoded transactional data\n",
    "reconstruction = decoder_eval(encoder_eval(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's assess its quality by calculating the reconstruction error over the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine reconstruction loss - all transactions\n",
    "reconstruction_loss_all = loss_function(reconstruction, data)\n",
    "\n",
    "# print reconstruction loss - all transactions\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] collected reconstruction loss of: {:06}/{:06} transactions'.format(now, reconstruction.size()[0], reconstruction.size()[0]))\n",
    "print('[LOG {}] reconstruction loss: {:.10f}'.format(now, reconstruction_loss_all.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, nice. Comparing the overall reconstruction loss of the pre-trained model trained for **20 epochs** to the one we initially trained for **5 epochs** reveals, that the pre-trained model results in a significantly lower reconstruction error. We can therefore conclude that the pre-trained model outperforms our initial model in capturing the inherent characteristics of the journal entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Assessment of the Individual Transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we are convinced that the pre-trained model is of descent quality let's no assess the individual journal entries of the dataset. To achieve this we collect the reconstruction errors of each individual journal entry by executing the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init binary cross entropy errors\n",
    "reconstruction_loss_transaction = np.zeros(reconstruction.size()[0])\n",
    "\n",
    "# iterate over all detailed reconstructions\n",
    "for i in range(0, reconstruction.size()[0]):\n",
    "\n",
    "    # determine reconstruction loss - individual transactions\n",
    "    reconstruction_loss_transaction[i] = loss_function(reconstruction[i], data[i]).data[0]\n",
    "\n",
    "    if(i % 100000 == 0):\n",
    "\n",
    "        ### print conversion summary\n",
    "        now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "        print('[LOG {}] collected individial reconstruction loss of: {:06}/{:06} transactions'.format(now, i, reconstruction.size()[0]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have collected individual reconstruction errors let's visualize them accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# ass \n",
    "plot_data = np.column_stack((np.arange(len(reconstruction_loss_transaction)), reconstruction_loss_transaction))\n",
    "\n",
    "# obtain regular transactions as well as global and local anomalies\n",
    "regular_data = plot_data[label == 'regular']\n",
    "global_outliers = plot_data[label == 'global']\n",
    "local_outliers = plot_data[label == 'local']\n",
    "\n",
    "# plot reconstruction error scatter plot\n",
    "ax.scatter(regular_data[:, 0], regular_data[:, 1], c='C0', alpha=0.4, marker=\"o\", label='regular') # plot regular transactions\n",
    "ax.scatter(global_outliers[:, 0], global_outliers[:, 1], c='C1', marker=\"^\", label='global') # plot global outliers\n",
    "ax.scatter(local_outliers[:, 0], local_outliers[:, 1], c='C2', marker=\"^\", label='local') # plot local outliers\n",
    "\n",
    "# add plot legend of transaction classes\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization reveals that the pre-trained model is able to reconstruct the majority of regular journal entries, while failing to do so for the anomalous ones. As a result the models reconstruction error can be used to distinguish both \"global\" anomalies (orange) and \"local\" anomalies (green) from the regular financial transactions (blue).\n",
    "\n",
    "To further investigate our observation and confirm our initial assumption, let's have a closer look into the transactions exhibiting a \"high\" binary cross-entropy reconstruction error >= 0.1. We assume that these transactions mostly correspond to the \"global\" anomalies of the original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append labels to original dataset\n",
    "ori_dataset['label'] = label\n",
    "\n",
    "# inspect transactions exhibiting a reconstruction error >= 0.1\n",
    "ori_dataset[reconstruction_loss_transaction >= 0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New let's also have a closer look into the transactions exhibiting a \"medium\" binary cross-entropy reconstruction error >= 0.02 and < 0.1. We assume that these transactions mostly correspond to the \"local\" anomalies of the original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect transactions exhibiting a reconstruction error < 0.1 and >= 0.02\n",
    "ori_dataset[(reconstruction_loss_transaction >= 0.02) & (reconstruction_loss_transaction < 0.1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optional Excercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please read ahead and only come back to these optional exercises if time permits.\n",
    "\n",
    "**1. Train the autoencoder model from scratch** [15 mins]\n",
    "\n",
    "First, change the # of training epochs to **30** in the correspond cell above. Second, put the starting learning rate back to **0.001**. Third, comment out the two line where the pre-trained model is loaded (under \"restore pre-trained model checkpoint\"). \n",
    "\n",
    "Re-run the autoencoder training using Kernel -> Restart & Run All menu. \n",
    "\n",
    "**2. What would happen if we remove a few fully-connected layers?** [15 mins]\n",
    "\n",
    "We designed a specific model for the lab because experiments show that the structure provided yield a good accuracy. Let's see how the reconstruction performance chances if we would **remove several of the hidden layers**. First, adjust the encoder and decoder model definitions accordingly (you may want to use the code snippets shown below). Then, follow all the instructions for training from scratch.\n",
    "\n",
    "Re-run the whole notebook using Kernel -> Restart & Run All menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of the shallow encoder network \n",
    "# containing only a single layer\n",
    "class encoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(encoder, self).__init__()\n",
    "\n",
    "        # specify layer 1 - in 186, out 3\n",
    "        self.encoder_L1 = nn.Linear(in_features=186, out_features=3, bias=True) # add linearity \n",
    "        nn.init.xavier_uniform(self.encoder_L1.weight) # init weights according to [6]\n",
    "        self.encoder_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [todo]\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # define forward pass through the network\n",
    "        x = self.encoder_R1(self.encoder_L1(x)) # don't apply dropout to the AE bottleneck\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of the shallow decoder network \n",
    "# containing only a single layer\n",
    "class decoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(decoder, self).__init__()\n",
    "\n",
    "        # specify layer 1 - in 3, out 8\n",
    "        self.decoder_L1 = nn.Linear(in_features=3, out_features=8, bias=True) # add linearity \n",
    "        nn.init.xavier_uniform(self.decoder_L1.weight)  # init weights according to [todo]\n",
    "        self.decoder_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [todo]\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # define forward pass through the network\n",
    "        x = self.decoder_R1(self.decoder_L1(x)) # don't apply dropout to the AE output\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Lab Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we presented a step by step implementation of an autoencoder deep neural network based methodology to detect anomalies in financial data. The the degree of a financial transaction abnormaty is evaluated based on its respective reconstruction error. The code provided in this lab can be tailored to meet more complex fraud detection scenarios and datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Post-Lab Excercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend you to try the following excercises after the lab:\n",
    "\n",
    "**1. Evaluation of shallow and deep autoencoder models** \n",
    "\n",
    "Try to train and evaluate further (shallow and deeper) autoencoder models (by removing and adding of fully-connected layers). Analyze the performance in terms of training time and reconstruction error.\n",
    "\n",
    "**2. Comparison to other dimensionality reduction techniques**\n",
    "\n",
    "Try using other dimensionality reduction techniques such as principal component analysis, non-negative matrix factorization or sparse coding and compare the detected anomalies with the ones detected by the autoencoder.\n",
    "\n",
    "**3. Review of additional autoencoder concepts**\n",
    "\n",
    "Try using other autoencoder architectures such as variational [10] or adversarial [11] autoencoder and compare the results with the autoencoder architecture implemented above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 10. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[1] ACFE, \"Report to the Nations on Occupational Fraud and Abuse\", The 2016 Global Fraud Study, Association of Certified Fraud Examiners (ACFE), 2016.\n",
    "\n",
    "[2] J. T. Wells, \"Corporate Fraud Handbook: Prevention and Detection\", John Wiley & Sons, 2017.\n",
    "\n",
    "[3] PwC, \"Pulling Fraud Out of the Shadows\", The Global Economic Crime and Fraud Survey 2018, PricewaterhouseCoopers LLP, 2018.\n",
    "\n",
    "[4] S. Markovitch, P. Willmott, \"Accelerating the digitization of business processes\", McKinsey & Company (2014) 1–5.\n",
    "\n",
    "[5] SAP, SAP Global Corporate Affairs, Corporate Factsheet 2017, 2017.\n",
    "\n",
    "[6] E. A. Lopez-Rojas , A. Elmir, and S. Axelsson, \"PaySim: A financial mobile money simulator for fraud detection\", In: The 28th European Modeling and Simulation Symposium-EMSS, Larnaca, Cyprus, 2016.\n",
    "\n",
    "[7] G. E. Hinton, and R. R. Salakhutdinov, \"Reducing the dimensionality of data with neural networks\", science 313, no. 5786: 504-507, 2006.\n",
    "\n",
    "[8] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, \"Dropout: A simple way to prevent neural networks from overfitting\", The Journal of Machine Learning Research, 15(1), 1929-1958, 2014.\n",
    "\n",
    "[9] X. Glorot and Y. Bengio, \"Understanding the difficulty of training deep feedforward neural networks\", Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS), 9:249–256, 2010.\n",
    "\n",
    "[10] B. Xu, N. Wang, T. Chen, and M. Li, \"Empirical Evaluation of Rectified Activations in Convolution Network\", ICML Deep Learning Workshop, pages 1–5, 2015.\n",
    "\n",
    "[11] D. P. Kingma and J. Ba, \"Adam: A method for stochastic optimization\", International Conference on Learning Representations (ICLR). 2015.\n",
    "\n",
    "[12] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, R. R. Salakhutdinov. \"Improving neural networks by preventing co-adaptation of feature detectors\", Technical Report, 2012.\n",
    "\n",
    "[13] D. P. Kingma, M. Welling. \"Auto-encoding variational bayes\", arXiv preprint arXiv:1312.6114, 2013.\n",
    "\n",
    "[14] Makhzani, A., Shlens, J., Jaitly, N., Goodfellow, I., & Frey, B., \"Adversarial autoencoders\", arXiv preprint arXiv:1511.05644, 2015.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
