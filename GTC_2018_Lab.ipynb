{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## \"Detection of Anomalies in Financial Transactions using Deep Autoencoder Networks\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This GPU Technology Conference (GTC) 2018 lab was developed by Mr. X, and Mr. Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Environment Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 01.1 Python Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, let's verify that Python is working on your system. To do this, execute the cell block below by giving it focus (clicking on it with your mouse), and hitting Shift-Enter, or pressing the play button in the toolbar above. If all goes well, you should see some output returned below the grey cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer should be forty-two: 42\n"
     ]
    }
   ],
   "source": [
    "print('The answer should be forty-two: {}'.format(str(40+2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 01.2 Import Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing utilities\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# importing pytorch libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import autograd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# importing data science libraries\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 01.3 CUDNN / GPU Verficiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CUDNN backend version: None\n"
     ]
    }
   ],
   "source": [
    "# print CUDNN backend version\n",
    "print('The CUDNN backend version: {}'.format(torch.backends.cudnn.version()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's execute the cell below to display information about the GPUs running on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: nvidia-smi: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's execute the cell below to display information about the PyTorch version running on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20180125-10:40:56] PyTorch version: 0.3.0.post4\n"
     ]
    }
   ],
   "source": [
    "# print current PyTorch version\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] PyTorch version: {}'.format(now, torch.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Lab Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo -- Timur and Marco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" style=\"max-width: 550px; height: auto\" src=\"images/accounting.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" style=\"max-width: 550px; height: auto\" src=\"images/anomalies.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" style=\"max-width: 400px; height: auto\" src=\"images/cube.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Autoencoder Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03.1 Introduction to Autoencoder Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" style=\"max-width: 600px; height: auto\" src=\"images/autoencoder.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03.2 Implementing the Encoder Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(encoder, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.0, inplace=True)\n",
    "\n",
    "        self.encoder_L1 = nn.Linear(401, 512, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L1.weight)\n",
    "        self.encoder_R1 = nn.LeakyReLU(negative_slope= 0.4, inplace=True)\n",
    "\n",
    "        self.encoder_L2 = nn.Linear(512, 256, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L2.weight)\n",
    "        self.encoder_R2 = nn.LeakyReLU(negative_slope= 0.4, inplace=True)\n",
    "\n",
    "        self.encoder_L3 = nn.Linear(256, 128, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L3.weight)\n",
    "        self.encoder_R3 = nn.LeakyReLU(negative_slope= 0.4, inplace=True)\n",
    "\n",
    "        self.encoder_L4 = nn.Linear(128, 64, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L4.weight)\n",
    "        self.encoder_R4 = nn.LeakyReLU(negative_slope= 0.4, inplace=True)\n",
    "\n",
    "        self.encoder_L5 = nn.Linear(64, 32, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L5.weight)\n",
    "        self.encoder_R5 = nn.LeakyReLU(negative_slope= 0.4, inplace=True)\n",
    "\n",
    "        self.encoder_L6 = nn.Linear(32, 16, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L6.weight)\n",
    "        self.encoder_R6 = nn.LeakyReLU(negative_slope= 0.4, inplace=True)\n",
    "\n",
    "        self.encoder_L7 = nn.Linear(16, 8, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L7.weight)\n",
    "        self.encoder_R7 = nn.LeakyReLU(negative_slope= 0.4, inplace=True)\n",
    "\n",
    "        self.encoder_L8 = nn.Linear(8, 4, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L8.weight)\n",
    "        self.encoder_R8 = nn.LeakyReLU(negative_slope= 0.4, inplace=True)\n",
    "\n",
    "        self.encoder_L9 = nn.Linear(4, 3, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L9.weight)\n",
    "        self.encoder_R9 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.encoder_R1(self.dropout(self.encoder_L1(x)))\n",
    "        x = self.encoder_R2(self.dropout(self.encoder_L2(x)))\n",
    "        x = self.encoder_R3(self.dropout(self.encoder_L3(x)))\n",
    "        x = self.encoder_R4(self.dropout(self.encoder_L4(x)))\n",
    "        x = self.encoder_R5(self.dropout(self.encoder_L5(x)))\n",
    "        x = self.encoder_R6(self.dropout(self.encoder_L6(x)))\n",
    "        x = self.encoder_R7(self.dropout(self.encoder_L7(x)))\n",
    "        x = self.encoder_R8(self.dropout(self.encoder_L8(x)))\n",
    "        x = self.encoder_R9(self.encoder_L9(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03.3 Implementing the Decoder Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(decoder, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.0, inplace=True)\n",
    "\n",
    "        self.decoder_L1 = nn.Linear(3, 4, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L1.weight)\n",
    "        self.decoder_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.decoder_L2 = nn.Linear(4, 8, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L2.weight)\n",
    "        self.decoder_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.decoder_L3 = nn.Linear(8, 16, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L3.weight)\n",
    "        self.decoder_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.decoder_L4 = nn.Linear(16, 32, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L4.weight)\n",
    "        self.decoder_R4 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.decoder_L5 = nn.Linear(32, 64, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L5.weight)\n",
    "        self.decoder_R5 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.decoder_L6 = nn.Linear(64, 128, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L6.weight)\n",
    "        self.decoder_R6 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.decoder_L7 = nn.Linear(128, 256, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L7.weight)\n",
    "        self.decoder_R7 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.decoder_L8 = nn.Linear(256, 512, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L8.weight)\n",
    "        self.decoder_R8 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        self.decoder_L9 = nn.Linear(512, 401, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L9.weight)\n",
    "        self.decoder_R9 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.decoder_R1(self.dropout(self.decoder_L1(x)))\n",
    "        x = self.decoder_R2(self.dropout(self.decoder_L2(x)))\n",
    "        x = self.decoder_R3(self.dropout(self.decoder_L3(x)))\n",
    "        x = self.decoder_R4(self.dropout(self.decoder_L4(x)))\n",
    "        x = self.decoder_R5(self.dropout(self.decoder_L5(x)))\n",
    "        x = self.decoder_R6(self.dropout(self.decoder_L6(x)))\n",
    "        x = self.decoder_R7(self.dropout(self.decoder_L7(x)))\n",
    "        x = self.decoder_R8(self.dropout(self.decoder_L8(x)))\n",
    "        x = self.decoder_R9(self.decoder_L9(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# init training network classes / architectures\n",
    "encoder_train = encoder()\n",
    "decoder_train = decoder()\n",
    "\n",
    "# push to cuda if cudnn is available\n",
    "if (torch.backends.cudnn.version() != None):\n",
    "    encoder_train = encoder().cuda()\n",
    "    decoder_train = decoder().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20180125-10:40:56] encoder architecture:\n",
      "\n",
      "encoder(\n",
      "  (dropout): Dropout(p=0.0, inplace)\n",
      "  (encoder_L1): Linear(in_features=401, out_features=512)\n",
      "  (encoder_R1): LeakyReLU(0.4, inplace)\n",
      "  (encoder_L2): Linear(in_features=512, out_features=256)\n",
      "  (encoder_R2): LeakyReLU(0.4, inplace)\n",
      "  (encoder_L3): Linear(in_features=256, out_features=128)\n",
      "  (encoder_R3): LeakyReLU(0.4, inplace)\n",
      "  (encoder_L4): Linear(in_features=128, out_features=64)\n",
      "  (encoder_R4): LeakyReLU(0.4, inplace)\n",
      "  (encoder_L5): Linear(in_features=64, out_features=32)\n",
      "  (encoder_R5): LeakyReLU(0.4, inplace)\n",
      "  (encoder_L6): Linear(in_features=32, out_features=16)\n",
      "  (encoder_R6): LeakyReLU(0.4, inplace)\n",
      "  (encoder_L7): Linear(in_features=16, out_features=8)\n",
      "  (encoder_R7): LeakyReLU(0.4, inplace)\n",
      "  (encoder_L8): Linear(in_features=8, out_features=4)\n",
      "  (encoder_R8): LeakyReLU(0.4, inplace)\n",
      "  (encoder_L9): Linear(in_features=4, out_features=3)\n",
      "  (encoder_R9): LeakyReLU(0.4, inplace)\n",
      ")\n",
      "\n",
      "[LOG 20180125-10:40:56] decoder architecture:\n",
      "\n",
      "decoder(\n",
      "  (dropout): Dropout(p=0.0, inplace)\n",
      "  (decoder_L1): Linear(in_features=3, out_features=4)\n",
      "  (decoder_R1): LeakyReLU(0.4, inplace)\n",
      "  (decoder_L2): Linear(in_features=4, out_features=8)\n",
      "  (decoder_R2): LeakyReLU(0.4, inplace)\n",
      "  (decoder_L3): Linear(in_features=8, out_features=16)\n",
      "  (decoder_R3): LeakyReLU(0.4, inplace)\n",
      "  (decoder_L4): Linear(in_features=16, out_features=32)\n",
      "  (decoder_R4): LeakyReLU(0.4, inplace)\n",
      "  (decoder_L5): Linear(in_features=32, out_features=64)\n",
      "  (decoder_R5): LeakyReLU(0.4, inplace)\n",
      "  (decoder_L6): Linear(in_features=64, out_features=128)\n",
      "  (decoder_R6): LeakyReLU(0.4, inplace)\n",
      "  (decoder_L7): Linear(in_features=128, out_features=256)\n",
      "  (decoder_R7): LeakyReLU(0.4, inplace)\n",
      "  (decoder_L8): Linear(in_features=256, out_features=512)\n",
      "  (decoder_R8): LeakyReLU(0.4, inplace)\n",
      "  (decoder_L9): Linear(in_features=512, out_features=401)\n",
      "  (decoder_R9): LeakyReLU(0.4, inplace)\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the initialized architectures\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] encoder architecture:\\n\\n{}\\n'.format(now, encoder_train))\n",
    "print('[LOG {}] decoder architecture:\\n\\n{}\\n'.format(now, decoder_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Financial Fraud Detection Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo -- Timur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20180125-10:41:18] encoded transactions of shape [307457/401] imported\n"
     ]
    }
   ],
   "source": [
    "# import original and encoded transactions\n",
    "ori_dataset = pd.read_csv(\"./data/transactions.csv\", sep=\",\", header=0, encoding=\"utf-8\")\n",
    "enc_dataset = pd.read_csv(\"./data/enc_transactions.csv\", sep=\",\", header=0, encoding=\"utf-8\").astype(float)\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print(\"[LOG {}] encoded transactions of shape [{}/{}] imported\".format(str(now), str(enc_dataset.shape[0]), str(enc_dataset.shape[1])))\n",
    "\n",
    "torch_dataset = torch.from_numpy(enc_dataset.values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_Key</th>\n",
       "      <th>Mapping_Key</th>\n",
       "      <th>NetValueInDocumentCurrency</th>\n",
       "      <th>TaxAmountInDocumentCurrency</th>\n",
       "      <th>ProductCode_Key</th>\n",
       "      <th>AccountID_Key</th>\n",
       "      <th>CurrencyCode_Key</th>\n",
       "      <th>TaxCode_Key</th>\n",
       "      <th>CompanyKey_Key</th>\n",
       "      <th>ShipToCountry_Key</th>\n",
       "      <th>ShipFromCountry_Key</th>\n",
       "      <th>CustomerID_ShipTo_Key</th>\n",
       "      <th>CustomerID_BillTo_Key</th>\n",
       "      <th>Artifical_Outlier</th>\n",
       "      <th>Reconstruction_Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>2016974D-CFB0-E611-8102-C4346BBA80C7</td>\n",
       "      <td>2201.60</td>\n",
       "      <td>154.11</td>\n",
       "      <td>84AEF4D4B7654CB681B9446DEF5BAD1A</td>\n",
       "      <td>5895263143C64D4CA01E21F319093F96</td>\n",
       "      <td>0F38C9CC1A6A4131BADDFE4C7935C7F3</td>\n",
       "      <td>CF7CD9ABACF94BA788D801B8D842D46C</td>\n",
       "      <td>873941E34B11498C83AC1FE7AFA64573</td>\n",
       "      <td>9718B8940BDE430BB42DD0F4697D4077</td>\n",
       "      <td>9718B8940BDE430BB42DD0F4697D4077</td>\n",
       "      <td>F743A173C9DB42F3A2BCA697811D65DB</td>\n",
       "      <td>FF3BA9F206DE45C0AE510E0EBEDA647C</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>FD16974D-CFB0-E611-8102-C4346BBA80C7</td>\n",
       "      <td>624.00</td>\n",
       "      <td>43.68</td>\n",
       "      <td>D56AEDEC0F5F43B1A47F0F6243AAF3A0</td>\n",
       "      <td>5895263143C64D4CA01E21F319093F96</td>\n",
       "      <td>0F38C9CC1A6A4131BADDFE4C7935C7F3</td>\n",
       "      <td>CF7CD9ABACF94BA788D801B8D842D46C</td>\n",
       "      <td>873941E34B11498C83AC1FE7AFA64573</td>\n",
       "      <td>9718B8940BDE430BB42DD0F4697D4077</td>\n",
       "      <td>9718B8940BDE430BB42DD0F4697D4077</td>\n",
       "      <td>F743A173C9DB42F3A2BCA697811D65DB</td>\n",
       "      <td>C2C47ED6F59D4956A1D1FF1CB576667C</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>0417974D-CFB0-E611-8102-C4346BBA80C7</td>\n",
       "      <td>387.20</td>\n",
       "      <td>27.10</td>\n",
       "      <td>98F22EC5A8F4403394995DE2F8348C73</td>\n",
       "      <td>5895263143C64D4CA01E21F319093F96</td>\n",
       "      <td>0F38C9CC1A6A4131BADDFE4C7935C7F3</td>\n",
       "      <td>CF7CD9ABACF94BA788D801B8D842D46C</td>\n",
       "      <td>873941E34B11498C83AC1FE7AFA64573</td>\n",
       "      <td>9718B8940BDE430BB42DD0F4697D4077</td>\n",
       "      <td>9718B8940BDE430BB42DD0F4697D4077</td>\n",
       "      <td>F743A173C9DB42F3A2BCA697811D65DB</td>\n",
       "      <td>99821244C8134F6BB32C834F5A69F26E</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>1717974D-CFB0-E611-8102-C4346BBA80C7</td>\n",
       "      <td>6704.55</td>\n",
       "      <td>469.32</td>\n",
       "      <td>7F175AD2FC5D41C6B8559F0153B4F481</td>\n",
       "      <td>5895263143C64D4CA01E21F319093F96</td>\n",
       "      <td>0F38C9CC1A6A4131BADDFE4C7935C7F3</td>\n",
       "      <td>CF7CD9ABACF94BA788D801B8D842D46C</td>\n",
       "      <td>873941E34B11498C83AC1FE7AFA64573</td>\n",
       "      <td>9718B8940BDE430BB42DD0F4697D4077</td>\n",
       "      <td>9718B8940BDE430BB42DD0F4697D4077</td>\n",
       "      <td>F743A173C9DB42F3A2BCA697811D65DB</td>\n",
       "      <td>197360EBA1F241D8919F6C134475DDC5</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>2017974D-CFB0-E611-8102-C4346BBA80C7</td>\n",
       "      <td>9374.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3A285A10662B4CB68BE2F159A189966D</td>\n",
       "      <td>BB637D3E330046DEA4A54465F2D51FBC</td>\n",
       "      <td>0F38C9CC1A6A4131BADDFE4C7935C7F3</td>\n",
       "      <td>4F60E5173B9B496097A2DC17BC0A1C75</td>\n",
       "      <td>873941E34B11498C83AC1FE7AFA64573</td>\n",
       "      <td>9718B8940BDE430BB42DD0F4697D4077</td>\n",
       "      <td>9718B8940BDE430BB42DD0F4697D4077</td>\n",
       "      <td>F743A173C9DB42F3A2BCA697811D65DB</td>\n",
       "      <td>FEBB9C2E391C47D9B583524875760628</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID_Key                           Mapping_Key  NetValueInDocumentCurrency  \\\n",
       "95      96  2016974D-CFB0-E611-8102-C4346BBA80C7                     2201.60   \n",
       "96      97  FD16974D-CFB0-E611-8102-C4346BBA80C7                      624.00   \n",
       "97      98  0417974D-CFB0-E611-8102-C4346BBA80C7                      387.20   \n",
       "98      99  1717974D-CFB0-E611-8102-C4346BBA80C7                     6704.55   \n",
       "99     100  2017974D-CFB0-E611-8102-C4346BBA80C7                     9374.40   \n",
       "\n",
       "    TaxAmountInDocumentCurrency                   ProductCode_Key  \\\n",
       "95                       154.11  84AEF4D4B7654CB681B9446DEF5BAD1A   \n",
       "96                        43.68  D56AEDEC0F5F43B1A47F0F6243AAF3A0   \n",
       "97                        27.10  98F22EC5A8F4403394995DE2F8348C73   \n",
       "98                       469.32  7F175AD2FC5D41C6B8559F0153B4F481   \n",
       "99                         0.00  3A285A10662B4CB68BE2F159A189966D   \n",
       "\n",
       "                       AccountID_Key                  CurrencyCode_Key  \\\n",
       "95  5895263143C64D4CA01E21F319093F96  0F38C9CC1A6A4131BADDFE4C7935C7F3   \n",
       "96  5895263143C64D4CA01E21F319093F96  0F38C9CC1A6A4131BADDFE4C7935C7F3   \n",
       "97  5895263143C64D4CA01E21F319093F96  0F38C9CC1A6A4131BADDFE4C7935C7F3   \n",
       "98  5895263143C64D4CA01E21F319093F96  0F38C9CC1A6A4131BADDFE4C7935C7F3   \n",
       "99  BB637D3E330046DEA4A54465F2D51FBC  0F38C9CC1A6A4131BADDFE4C7935C7F3   \n",
       "\n",
       "                         TaxCode_Key                    CompanyKey_Key  \\\n",
       "95  CF7CD9ABACF94BA788D801B8D842D46C  873941E34B11498C83AC1FE7AFA64573   \n",
       "96  CF7CD9ABACF94BA788D801B8D842D46C  873941E34B11498C83AC1FE7AFA64573   \n",
       "97  CF7CD9ABACF94BA788D801B8D842D46C  873941E34B11498C83AC1FE7AFA64573   \n",
       "98  CF7CD9ABACF94BA788D801B8D842D46C  873941E34B11498C83AC1FE7AFA64573   \n",
       "99  4F60E5173B9B496097A2DC17BC0A1C75  873941E34B11498C83AC1FE7AFA64573   \n",
       "\n",
       "                   ShipToCountry_Key               ShipFromCountry_Key  \\\n",
       "95  9718B8940BDE430BB42DD0F4697D4077  9718B8940BDE430BB42DD0F4697D4077   \n",
       "96  9718B8940BDE430BB42DD0F4697D4077  9718B8940BDE430BB42DD0F4697D4077   \n",
       "97  9718B8940BDE430BB42DD0F4697D4077  9718B8940BDE430BB42DD0F4697D4077   \n",
       "98  9718B8940BDE430BB42DD0F4697D4077  9718B8940BDE430BB42DD0F4697D4077   \n",
       "99  9718B8940BDE430BB42DD0F4697D4077  9718B8940BDE430BB42DD0F4697D4077   \n",
       "\n",
       "               CustomerID_ShipTo_Key             CustomerID_BillTo_Key  \\\n",
       "95  F743A173C9DB42F3A2BCA697811D65DB  FF3BA9F206DE45C0AE510E0EBEDA647C   \n",
       "96  F743A173C9DB42F3A2BCA697811D65DB  C2C47ED6F59D4956A1D1FF1CB576667C   \n",
       "97  F743A173C9DB42F3A2BCA697811D65DB  99821244C8134F6BB32C834F5A69F26E   \n",
       "98  F743A173C9DB42F3A2BCA697811D65DB  197360EBA1F241D8919F6C134475DDC5   \n",
       "99  F743A173C9DB42F3A2BCA697811D65DB  FEBB9C2E391C47D9B583524875760628   \n",
       "\n",
       "    Artifical_Outlier  Reconstruction_Error  \n",
       "95                  0                   NaN  \n",
       "96                  0                   NaN  \n",
       "97                  0                   NaN  \n",
       "98                  0                   NaN  \n",
       "99                  0                   NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sneak preview - regular transactions\n",
    "ori_dataset[ori_dataset.Artifical_Outlier == 0].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_Key</th>\n",
       "      <th>Mapping_Key</th>\n",
       "      <th>NetValueInDocumentCurrency</th>\n",
       "      <th>TaxAmountInDocumentCurrency</th>\n",
       "      <th>ProductCode_Key</th>\n",
       "      <th>AccountID_Key</th>\n",
       "      <th>CurrencyCode_Key</th>\n",
       "      <th>TaxCode_Key</th>\n",
       "      <th>CompanyKey_Key</th>\n",
       "      <th>ShipToCountry_Key</th>\n",
       "      <th>ShipFromCountry_Key</th>\n",
       "      <th>CustomerID_ShipTo_Key</th>\n",
       "      <th>CustomerID_BillTo_Key</th>\n",
       "      <th>Artifical_Outlier</th>\n",
       "      <th>Reconstruction_Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>999999.0</td>\n",
       "      <td>50000.0000</td>\n",
       "      <td>Aplha</td>\n",
       "      <td>Bravo</td>\n",
       "      <td>Tango</td>\n",
       "      <td>Fruti</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>India</td>\n",
       "      <td>Anant</td>\n",
       "      <td>Marco</td>\n",
       "      <td>Bermuda</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>598434.0</td>\n",
       "      <td>88000.0000</td>\n",
       "      <td>Zori</td>\n",
       "      <td>Account</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>Python</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>details</td>\n",
       "      <td>?ro7ekt</td>\n",
       "      <td>placed</td>\n",
       "      <td>SOurdf</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>450864.0</td>\n",
       "      <td>70005.0000</td>\n",
       "      <td>Author</td>\n",
       "      <td>Book</td>\n",
       "      <td>What??</td>\n",
       "      <td>DUH!!</td>\n",
       "      <td>I</td>\n",
       "      <td>Hate</td>\n",
       "      <td>these</td>\n",
       "      <td>Anamolies</td>\n",
       "      <td>Detection</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108888.0</td>\n",
       "      <td>28000.0000</td>\n",
       "      <td>top</td>\n",
       "      <td>what!</td>\n",
       "      <td>AyCaramba!</td>\n",
       "      <td>PwC</td>\n",
       "      <td>FTS</td>\n",
       "      <td>=Great</td>\n",
       "      <td>Maybe?</td>\n",
       "      <td>Artificial</td>\n",
       "      <td>Outliers</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>233355.0</td>\n",
       "      <td>4514.5617</td>\n",
       "      <td>Java</td>\n",
       "      <td>Progrramming</td>\n",
       "      <td>Sucks</td>\n",
       "      <td>really.</td>\n",
       "      <td>Go</td>\n",
       "      <td>Kalrsruhe!</td>\n",
       "      <td>I</td>\n",
       "      <td>am</td>\n",
       "      <td>dumb</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_Key Mapping_Key  NetValueInDocumentCurrency  \\\n",
       "0       1         NaN                    999999.0   \n",
       "1       2         NaN                    598434.0   \n",
       "2       3         NaN                    450864.0   \n",
       "3       4         NaN                    108888.0   \n",
       "4       5         NaN                    233355.0   \n",
       "\n",
       "   TaxAmountInDocumentCurrency ProductCode_Key AccountID_Key CurrencyCode_Key  \\\n",
       "0                   50000.0000           Aplha         Bravo            Tango   \n",
       "1                   88000.0000            Zori       Account      Categorical   \n",
       "2                   70005.0000          Author          Book           What??   \n",
       "3                   28000.0000             top         what!       AyCaramba!   \n",
       "4                    4514.5617            Java  Progrramming            Sucks   \n",
       "\n",
       "  TaxCode_Key CompanyKey_Key ShipToCountry_Key ShipFromCountry_Key  \\\n",
       "0       Fruti          Kenya             India               Anant   \n",
       "1      Python      Indonesia           details             ?ro7ekt   \n",
       "2       DUH!!              I              Hate               these   \n",
       "3         PwC            FTS            =Great              Maybe?   \n",
       "4     really.             Go        Kalrsruhe!                   I   \n",
       "\n",
       "  CustomerID_ShipTo_Key CustomerID_BillTo_Key  Artifical_Outlier  \\\n",
       "0                 Marco               Bermuda                  1   \n",
       "1                placed                SOurdf                  1   \n",
       "2             Anamolies             Detection                  1   \n",
       "3            Artificial              Outliers                  1   \n",
       "4                    am                  dumb                  1   \n",
       "\n",
       "   Reconstruction_Error  \n",
       "0                   NaN  \n",
       "1                   NaN  \n",
       "2                   NaN  \n",
       "3                   NaN  \n",
       "4                   NaN  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sneak preview - global anomalies\n",
    "ori_dataset[ori_dataset.Artifical_Outlier == 1].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_Key</th>\n",
       "      <th>Mapping_Key</th>\n",
       "      <th>NetValueInDocumentCurrency</th>\n",
       "      <th>TaxAmountInDocumentCurrency</th>\n",
       "      <th>ProductCode_Key</th>\n",
       "      <th>AccountID_Key</th>\n",
       "      <th>CurrencyCode_Key</th>\n",
       "      <th>TaxCode_Key</th>\n",
       "      <th>CompanyKey_Key</th>\n",
       "      <th>ShipToCountry_Key</th>\n",
       "      <th>ShipFromCountry_Key</th>\n",
       "      <th>CustomerID_ShipTo_Key</th>\n",
       "      <th>CustomerID_BillTo_Key</th>\n",
       "      <th>Artifical_Outlier</th>\n",
       "      <th>Reconstruction_Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>555.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A11EB094B4E04987B8FE9B41D3BBDC65</td>\n",
       "      <td>BB637D3E330046DEA4A54465F2D51FBC</td>\n",
       "      <td>0F38C9CC1A6A4131BADDFE4C7935C7F3</td>\n",
       "      <td>4F60E5173B9B496097A2DC17BC0A1C75</td>\n",
       "      <td>7A3D4CB8036C4A4F9F6D44A58F65C1B4</td>\n",
       "      <td>EB5A18AA00F24ADFA7EE848E8172B7DA</td>\n",
       "      <td>EB5A18AA00F24ADFA7EE848E8172B7DA</td>\n",
       "      <td>F743A173C9DB42F3A2BCA697811D65DB</td>\n",
       "      <td>852ED2D345FF4D8EBC9910F674E6F906</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A11EB094B4E04987B8FE9B41D3BBDC65</td>\n",
       "      <td>BB637D3E330046DEA4A54465F2D51FBC</td>\n",
       "      <td>0F38C9CC1A6A4131BADDFE4C7935C7F3</td>\n",
       "      <td>4F60E5173B9B496097A2DC17BC0A1C75</td>\n",
       "      <td>873941E34B11498C83AC1FE7AFA64573</td>\n",
       "      <td>9718B8940BDE430BB42DD0F4697D4077</td>\n",
       "      <td>EB5A18AA00F24ADFA7EE848E8172B7DA</td>\n",
       "      <td>F743A173C9DB42F3A2BCA697811D65DB</td>\n",
       "      <td>852ED2D345FF4D8EBC9910F674E6F906</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>777.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>A11EB094B4E04987B8FE9B41D3BBDC65</td>\n",
       "      <td>BB637D3E330046DEA4A54465F2D51FBC</td>\n",
       "      <td>0F38C9CC1A6A4131BADDFE4C7935C7F3</td>\n",
       "      <td>4F60E5173B9B496097A2DC17BC0A1C75</td>\n",
       "      <td>7A3D4CB8036C4A4F9F6D44A58F65C1B4</td>\n",
       "      <td>42B1FF09883A43568B08AF79C484EC5A</td>\n",
       "      <td>9718B8940BDE430BB42DD0F4697D4077</td>\n",
       "      <td>F743A173C9DB42F3A2BCA697811D65DB</td>\n",
       "      <td>3F8A5C28E15F4051BF5E4A3581B57ED9</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>222.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>A11EB094B4E04987B8FE9B41D3BBDC65</td>\n",
       "      <td>08ED6A4395014426A7634B1BE3FC0426</td>\n",
       "      <td>0F38C9CC1A6A4131BADDFE4C7935C7F3</td>\n",
       "      <td>CF7CD9ABACF94BA788D801B8D842D46C</td>\n",
       "      <td>873941E34B11498C83AC1FE7AFA64573</td>\n",
       "      <td>42B1FF09883A43568B08AF79C484EC5A</td>\n",
       "      <td>EB5A18AA00F24ADFA7EE848E8172B7DA</td>\n",
       "      <td>F743A173C9DB42F3A2BCA697811D65DB</td>\n",
       "      <td>852ED2D345FF4D8EBC9910F674E6F906</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>333.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>A11EB094B4E04987B8FE9B41D3BBDC65</td>\n",
       "      <td>08ED6A4395014426A7634B1BE3FC0426</td>\n",
       "      <td>5D86A49D44E24CF2BBA401809115CBFD</td>\n",
       "      <td>CF7CD9ABACF94BA788D801B8D842D46C</td>\n",
       "      <td>7A3D4CB8036C4A4F9F6D44A58F65C1B4</td>\n",
       "      <td>42B1FF09883A43568B08AF79C484EC5A</td>\n",
       "      <td>EB5A18AA00F24ADFA7EE848E8172B7DA</td>\n",
       "      <td>F743A173C9DB42F3A2BCA697811D65DB</td>\n",
       "      <td>852ED2D345FF4D8EBC9910F674E6F906</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID_Key Mapping_Key  NetValueInDocumentCurrency  \\\n",
       "55      56         NaN                       555.0   \n",
       "56      57         NaN                        66.0   \n",
       "57      58         NaN                       777.0   \n",
       "58      59         NaN                       222.0   \n",
       "59      60         NaN                       333.0   \n",
       "\n",
       "    TaxAmountInDocumentCurrency                   ProductCode_Key  \\\n",
       "55                          5.0  A11EB094B4E04987B8FE9B41D3BBDC65   \n",
       "56                          3.0  A11EB094B4E04987B8FE9B41D3BBDC65   \n",
       "57                          4.0  A11EB094B4E04987B8FE9B41D3BBDC65   \n",
       "58                          9.0  A11EB094B4E04987B8FE9B41D3BBDC65   \n",
       "59                          4.0  A11EB094B4E04987B8FE9B41D3BBDC65   \n",
       "\n",
       "                       AccountID_Key                  CurrencyCode_Key  \\\n",
       "55  BB637D3E330046DEA4A54465F2D51FBC  0F38C9CC1A6A4131BADDFE4C7935C7F3   \n",
       "56  BB637D3E330046DEA4A54465F2D51FBC  0F38C9CC1A6A4131BADDFE4C7935C7F3   \n",
       "57  BB637D3E330046DEA4A54465F2D51FBC  0F38C9CC1A6A4131BADDFE4C7935C7F3   \n",
       "58  08ED6A4395014426A7634B1BE3FC0426  0F38C9CC1A6A4131BADDFE4C7935C7F3   \n",
       "59  08ED6A4395014426A7634B1BE3FC0426  5D86A49D44E24CF2BBA401809115CBFD   \n",
       "\n",
       "                         TaxCode_Key                    CompanyKey_Key  \\\n",
       "55  4F60E5173B9B496097A2DC17BC0A1C75  7A3D4CB8036C4A4F9F6D44A58F65C1B4   \n",
       "56  4F60E5173B9B496097A2DC17BC0A1C75  873941E34B11498C83AC1FE7AFA64573   \n",
       "57  4F60E5173B9B496097A2DC17BC0A1C75  7A3D4CB8036C4A4F9F6D44A58F65C1B4   \n",
       "58  CF7CD9ABACF94BA788D801B8D842D46C  873941E34B11498C83AC1FE7AFA64573   \n",
       "59  CF7CD9ABACF94BA788D801B8D842D46C  7A3D4CB8036C4A4F9F6D44A58F65C1B4   \n",
       "\n",
       "                   ShipToCountry_Key               ShipFromCountry_Key  \\\n",
       "55  EB5A18AA00F24ADFA7EE848E8172B7DA  EB5A18AA00F24ADFA7EE848E8172B7DA   \n",
       "56  9718B8940BDE430BB42DD0F4697D4077  EB5A18AA00F24ADFA7EE848E8172B7DA   \n",
       "57  42B1FF09883A43568B08AF79C484EC5A  9718B8940BDE430BB42DD0F4697D4077   \n",
       "58  42B1FF09883A43568B08AF79C484EC5A  EB5A18AA00F24ADFA7EE848E8172B7DA   \n",
       "59  42B1FF09883A43568B08AF79C484EC5A  EB5A18AA00F24ADFA7EE848E8172B7DA   \n",
       "\n",
       "               CustomerID_ShipTo_Key             CustomerID_BillTo_Key  \\\n",
       "55  F743A173C9DB42F3A2BCA697811D65DB  852ED2D345FF4D8EBC9910F674E6F906   \n",
       "56  F743A173C9DB42F3A2BCA697811D65DB  852ED2D345FF4D8EBC9910F674E6F906   \n",
       "57  F743A173C9DB42F3A2BCA697811D65DB  3F8A5C28E15F4051BF5E4A3581B57ED9   \n",
       "58  F743A173C9DB42F3A2BCA697811D65DB  852ED2D345FF4D8EBC9910F674E6F906   \n",
       "59  F743A173C9DB42F3A2BCA697811D65DB  852ED2D345FF4D8EBC9910F674E6F906   \n",
       "\n",
       "    Artifical_Outlier  Reconstruction_Error  \n",
       "55                  2                   NaN  \n",
       "56                  2                   NaN  \n",
       "57                  2                   NaN  \n",
       "58                  2                   NaN  \n",
       "59                  2                   NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sneak preview - local anomalies\n",
    "ori_dataset[ori_dataset.Artifical_Outlier == 2].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = [\"AccountID_Key\", \"CurrencyCode_Key\", \"TaxCode_Key\", \"CompanyKey_Key\", \"ShipToCountry_Key\", \"ShipFromCountry_Key\"]\n",
    "ranges = [0, 62+1, 121+1, 183+1, 234+1, 349+1, 400+1]\n",
    "\n",
    "# init training results\n",
    "columns = [\"timestamp\", \"node\", \"seed\", \"architecture\", \"epoch\", \"rec_loss\", \"roc_auc\", \"anomalies\", \"normalies\", \"anomalies_s\", \"normalies_s\", \"max_threshold\", \"max_tpr_s\", \"max_fpr_s\", \"precision\", \"recall\", \"f1_score\", \"fpr\", \"tpr\", \"thresholds\"]\n",
    "evaluations = [\"err_\" + str(element) for element in range(0, (len(features))+1)]\n",
    "columns.extend(evaluations)\n",
    "evaluations = [\"ano_c1_\" + str(element) for element in range(0, (len(features))+1)]\n",
    "columns.extend(evaluations)\n",
    "evaluations = [\"ano_c2_\" + str(element) for element in range(0, (len(features))+1)]\n",
    "columns.extend(evaluations)\n",
    "evaluation_results = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05. Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo - Timur and Marco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20180125-10_41_22] training status, epoch: [0001/0050], batch: 0100, loss: 0.0276836734\n",
      "[LOG 20180125-10_41_25] training status, epoch: [0001/0050], batch: 0200, loss: 0.0164137110\n",
      "[LOG 20180125-10_41_28] training status, epoch: [0001/0050], batch: 0300, loss: 0.0097901132\n",
      "[LOG 20180125-10_41_31] training status, epoch: [0001/0050], batch: 0400, loss: 0.0089493077\n",
      "[LOG 20180125-10_41_35] training status, epoch: [0001/0050], batch: 0500, loss: 0.0070521147\n",
      "[LOG 20180125-10_41_38] training status, epoch: [0001/0050], batch: 0600, loss: 0.0065270187\n",
      "[LOG 20180125-10_41_41] training status, epoch: [0001/0050], batch: 0700, loss: 0.0068099638\n",
      "[LOG 20180125-10_41_44] training status, epoch: [0001/0050], batch: 0800, loss: 0.0046783471\n",
      "[LOG 20180125-10_41_47] training status, epoch: [0001/0050], batch: 0900, loss: 0.0035287559\n",
      "[LOG 20180125-10_41_51] training status, epoch: [0001/0050], batch: 1000, loss: 0.0032302369\n",
      "[LOG 20180125-10_41_54] training status, epoch: [0001/0050], batch: 1100, loss: 0.0028442524\n",
      "[LOG 20180125-10_41_57] training status, epoch: [0001/0050], batch: 1200, loss: 0.0025757779\n",
      "[LOG 20180125-10_42_00] training status, epoch: [0001/0050], batch: 1300, loss: 0.0016700419\n",
      "[LOG 20180125-10_42_03] training status, epoch: [0001/0050], batch: 1400, loss: 0.0021013925\n",
      "[LOG 20180125-10_42_06] training status, epoch: [0001/0050], batch: 1500, loss: 0.0018651444\n",
      "[LOG 20180125-10_42_09] training status, epoch: [0001/0050], batch: 1600, loss: 0.0012738521\n",
      "[LOG 20180125-10_42_12] training status, epoch: [0001/0050], batch: 1700, loss: 0.0028924164\n",
      "[LOG 20180125-10_42_16] training status, epoch: [0001/0050], batch: 1800, loss: 0.0011395825\n",
      "[LOG 20180125-10_42_19] training status, epoch: [0001/0050], batch: 1900, loss: 0.0014132885\n",
      "[LOG 20180125-10_42_23] training status, epoch: [0001/0050], batch: 2000, loss: 0.0047396384\n",
      "[LOG 20180125-10_42_26] training status, epoch: [0001/0050], batch: 2100, loss: 0.0023968841\n",
      "[LOG 20180125-10_42_29] training status, epoch: [0001/0050], batch: 2200, loss: 0.0010263530\n",
      "[LOG 20180125-10_42_32] training status, epoch: [0001/0050], batch: 2300, loss: 0.0013717123\n",
      "[LOG 20180125-10_42_36] training status, epoch: [0001/0050], batch: 2400, loss: 0.0025664293\n",
      "[LOG 20180125-10_42_41] training status, epoch: [0002/0050], batch: 0100, loss: 0.0012187398\n",
      "[LOG 20180125-10_42_44] training status, epoch: [0002/0050], batch: 0200, loss: 0.0008528276\n",
      "[LOG 20180125-10_42_47] training status, epoch: [0002/0050], batch: 0300, loss: 0.0011884848\n",
      "[LOG 20180125-10_42_50] training status, epoch: [0002/0050], batch: 0400, loss: 0.0018860047\n",
      "[LOG 20180125-10_42_53] training status, epoch: [0002/0050], batch: 0500, loss: 0.0004705974\n",
      "[LOG 20180125-10_42_56] training status, epoch: [0002/0050], batch: 0600, loss: 0.0013202998\n",
      "[LOG 20180125-10_43_00] training status, epoch: [0002/0050], batch: 0700, loss: 0.0005104011\n",
      "[LOG 20180125-10_43_04] training status, epoch: [0002/0050], batch: 0800, loss: 0.0008544604\n",
      "[LOG 20180125-10_43_07] training status, epoch: [0002/0050], batch: 0900, loss: 0.0005596065\n",
      "[LOG 20180125-10_43_10] training status, epoch: [0002/0050], batch: 1000, loss: 0.0009034507\n",
      "[LOG 20180125-10_43_13] training status, epoch: [0002/0050], batch: 1100, loss: 0.0007258510\n",
      "[LOG 20180125-10_43_16] training status, epoch: [0002/0050], batch: 1200, loss: 0.0011164970\n",
      "[LOG 20180125-10_43_19] training status, epoch: [0002/0050], batch: 1300, loss: 0.0008356959\n",
      "[LOG 20180125-10_43_22] training status, epoch: [0002/0050], batch: 1400, loss: 0.0085732099\n",
      "[LOG 20180125-10_43_25] training status, epoch: [0002/0050], batch: 1500, loss: 0.0006568257\n",
      "[LOG 20180125-10_43_29] training status, epoch: [0002/0050], batch: 1600, loss: 0.0005228643\n",
      "[LOG 20180125-10_43_32] training status, epoch: [0002/0050], batch: 1700, loss: 0.0005337142\n",
      "[LOG 20180125-10_43_36] training status, epoch: [0002/0050], batch: 1800, loss: 0.0002307869\n",
      "[LOG 20180125-10_43_39] training status, epoch: [0002/0050], batch: 1900, loss: 0.0004037334\n",
      "[LOG 20180125-10_43_42] training status, epoch: [0002/0050], batch: 2000, loss: 0.0003861100\n",
      "[LOG 20180125-10_43_47] training status, epoch: [0002/0050], batch: 2100, loss: 0.0010601981\n",
      "[LOG 20180125-10_43_51] training status, epoch: [0002/0050], batch: 2200, loss: 0.0007486668\n",
      "[LOG 20180125-10_43_56] training status, epoch: [0002/0050], batch: 2300, loss: 0.0007093073\n",
      "[LOG 20180125-10_43_59] training status, epoch: [0002/0050], batch: 2400, loss: 0.0006668547\n",
      "[LOG 20180125-10_44_02] training status, epoch: [0003/0050], batch: 0100, loss: 0.0005526498\n",
      "[LOG 20180125-10_44_05] training status, epoch: [0003/0050], batch: 0200, loss: 0.0003688821\n",
      "[LOG 20180125-10_44_08] training status, epoch: [0003/0050], batch: 0300, loss: 0.0003782734\n",
      "[LOG 20180125-10_44_12] training status, epoch: [0003/0050], batch: 0400, loss: 0.0003824474\n",
      "[LOG 20180125-10_44_15] training status, epoch: [0003/0050], batch: 0500, loss: 0.0007702383\n",
      "[LOG 20180125-10_44_18] training status, epoch: [0003/0050], batch: 0600, loss: 0.0012437473\n",
      "[LOG 20180125-10_44_21] training status, epoch: [0003/0050], batch: 0700, loss: 0.0002878600\n",
      "[LOG 20180125-10_44_24] training status, epoch: [0003/0050], batch: 0800, loss: 0.0003718447\n",
      "[LOG 20180125-10_44_28] training status, epoch: [0003/0050], batch: 0900, loss: 0.0004181437\n",
      "[LOG 20180125-10_44_31] training status, epoch: [0003/0050], batch: 1000, loss: 0.0002871791\n",
      "[LOG 20180125-10_44_34] training status, epoch: [0003/0050], batch: 1100, loss: 0.0003787474\n",
      "[LOG 20180125-10_44_38] training status, epoch: [0003/0050], batch: 1200, loss: 0.0009461626\n",
      "[LOG 20180125-10_44_41] training status, epoch: [0003/0050], batch: 1300, loss: 0.0009850927\n",
      "[LOG 20180125-10_44_44] training status, epoch: [0003/0050], batch: 1400, loss: 0.0003812812\n",
      "[LOG 20180125-10_44_47] training status, epoch: [0003/0050], batch: 1500, loss: 0.0002633700\n",
      "[LOG 20180125-10_44_50] training status, epoch: [0003/0050], batch: 1600, loss: 0.0001285762\n",
      "[LOG 20180125-10_44_53] training status, epoch: [0003/0050], batch: 1700, loss: 0.0074184448\n",
      "[LOG 20180125-10_44_57] training status, epoch: [0003/0050], batch: 1800, loss: 0.0013495401\n",
      "[LOG 20180125-10_45_00] training status, epoch: [0003/0050], batch: 1900, loss: 0.0012646687\n",
      "[LOG 20180125-10_45_04] training status, epoch: [0003/0050], batch: 2000, loss: 0.0012898190\n",
      "[LOG 20180125-10_45_08] training status, epoch: [0003/0050], batch: 2100, loss: 0.0007481331\n",
      "[LOG 20180125-10_45_12] training status, epoch: [0003/0050], batch: 2200, loss: 0.0004687150\n",
      "[LOG 20180125-10_45_16] training status, epoch: [0003/0050], batch: 2300, loss: 0.0030019728\n",
      "[LOG 20180125-10_45_19] training status, epoch: [0003/0050], batch: 2400, loss: 0.0002684336\n",
      "[LOG 20180125-10_45_23] training status, epoch: [0004/0050], batch: 0100, loss: 0.0029433037\n",
      "[LOG 20180125-10_45_27] training status, epoch: [0004/0050], batch: 0200, loss: 0.0001954395\n",
      "[LOG 20180125-10_45_30] training status, epoch: [0004/0050], batch: 0300, loss: 0.0008272437\n",
      "[LOG 20180125-10_45_33] training status, epoch: [0004/0050], batch: 0400, loss: 0.0005507097\n",
      "[LOG 20180125-10_45_37] training status, epoch: [0004/0050], batch: 0500, loss: 0.0005396063\n",
      "[LOG 20180125-10_45_41] training status, epoch: [0004/0050], batch: 0600, loss: 0.0006584254\n",
      "[LOG 20180125-10_45_44] training status, epoch: [0004/0050], batch: 0700, loss: 0.0002658464\n",
      "[LOG 20180125-10_45_47] training status, epoch: [0004/0050], batch: 0800, loss: 0.0004736430\n",
      "[LOG 20180125-10_45_50] training status, epoch: [0004/0050], batch: 0900, loss: 0.0002146531\n",
      "[LOG 20180125-10_45_53] training status, epoch: [0004/0050], batch: 1000, loss: 0.0007052231\n",
      "[LOG 20180125-10_45_56] training status, epoch: [0004/0050], batch: 1100, loss: 0.0001237086\n",
      "[LOG 20180125-10_46_00] training status, epoch: [0004/0050], batch: 1200, loss: 0.0055154180\n",
      "[LOG 20180125-10_46_03] training status, epoch: [0004/0050], batch: 1300, loss: 0.0009518904\n",
      "[LOG 20180125-10_46_07] training status, epoch: [0004/0050], batch: 1400, loss: 0.0004310817\n",
      "[LOG 20180125-10_46_11] training status, epoch: [0004/0050], batch: 1500, loss: 0.0004568693\n",
      "[LOG 20180125-10_46_16] training status, epoch: [0004/0050], batch: 1600, loss: 0.0005846720\n",
      "[LOG 20180125-10_46_19] training status, epoch: [0004/0050], batch: 1700, loss: 0.0002116528\n",
      "[LOG 20180125-10_46_23] training status, epoch: [0004/0050], batch: 1800, loss: 0.0002205543\n",
      "[LOG 20180125-10_46_27] training status, epoch: [0004/0050], batch: 1900, loss: 0.0001059713\n",
      "[LOG 20180125-10_46_30] training status, epoch: [0004/0050], batch: 2000, loss: 0.0003462062\n",
      "[LOG 20180125-10_46_33] training status, epoch: [0004/0050], batch: 2100, loss: 0.0022171671\n",
      "[LOG 20180125-10_46_36] training status, epoch: [0004/0050], batch: 2200, loss: 0.0004765192\n",
      "[LOG 20180125-10_46_39] training status, epoch: [0004/0050], batch: 2300, loss: 0.0001362750\n",
      "[LOG 20180125-10_46_43] training status, epoch: [0004/0050], batch: 2400, loss: 0.0004206368\n",
      "[LOG 20180125-10_46_46] training status, epoch: [0005/0050], batch: 0100, loss: 0.0003765184\n",
      "[LOG 20180125-10_46_49] training status, epoch: [0005/0050], batch: 0200, loss: 0.0001060238\n",
      "[LOG 20180125-10_46_53] training status, epoch: [0005/0050], batch: 0300, loss: 0.0002646401\n",
      "[LOG 20180125-10_46_56] training status, epoch: [0005/0050], batch: 0400, loss: 0.0011348967\n",
      "[LOG 20180125-10_46_59] training status, epoch: [0005/0050], batch: 0500, loss: 0.0003802339\n",
      "[LOG 20180125-10_47_02] training status, epoch: [0005/0050], batch: 0600, loss: 0.0006615896\n",
      "[LOG 20180125-10_47_05] training status, epoch: [0005/0050], batch: 0700, loss: 0.0006538770\n",
      "[LOG 20180125-10_47_08] training status, epoch: [0005/0050], batch: 0800, loss: 0.0003699796\n",
      "[LOG 20180125-10_47_11] training status, epoch: [0005/0050], batch: 0900, loss: 0.0001157143\n",
      "[LOG 20180125-10_47_14] training status, epoch: [0005/0050], batch: 1000, loss: 0.0005893763\n",
      "[LOG 20180125-10_47_17] training status, epoch: [0005/0050], batch: 1100, loss: 0.0007899874\n",
      "[LOG 20180125-10_47_21] training status, epoch: [0005/0050], batch: 1200, loss: 0.0002541281\n",
      "[LOG 20180125-10_47_24] training status, epoch: [0005/0050], batch: 1300, loss: 0.0002034520\n",
      "[LOG 20180125-10_47_27] training status, epoch: [0005/0050], batch: 1400, loss: 0.0001339715\n",
      "[LOG 20180125-10_47_30] training status, epoch: [0005/0050], batch: 1500, loss: 0.0001348671\n",
      "[LOG 20180125-10_47_34] training status, epoch: [0005/0050], batch: 1600, loss: 0.0002927910\n",
      "[LOG 20180125-10_47_37] training status, epoch: [0005/0050], batch: 1700, loss: 0.0003606705\n",
      "[LOG 20180125-10_47_43] training status, epoch: [0005/0050], batch: 1800, loss: 0.0019917705\n",
      "[LOG 20180125-10_47_50] training status, epoch: [0005/0050], batch: 1900, loss: 0.0005159733\n",
      "[LOG 20180125-10_47_58] training status, epoch: [0005/0050], batch: 2000, loss: 0.0005424187\n",
      "[LOG 20180125-10_48_02] training status, epoch: [0005/0050], batch: 2100, loss: 0.0001729078\n",
      "[LOG 20180125-10_48_08] training status, epoch: [0005/0050], batch: 2200, loss: 0.0002307336\n",
      "[LOG 20180125-10_48_16] training status, epoch: [0005/0050], batch: 2300, loss: 0.0014784210\n",
      "[LOG 20180125-10_48_20] training status, epoch: [0005/0050], batch: 2400, loss: 0.0000517191\n",
      "[LOG 20180125-10_48_29] training status, epoch: [0006/0050], batch: 0100, loss: 0.0003553797\n",
      "[LOG 20180125-10_48_36] training status, epoch: [0006/0050], batch: 0200, loss: 0.0001084608\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2c7f17d36594>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# run backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mreconstruction_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# update network parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# init deterministic seed\n",
    "seed_value = 1234 #4444 #3333 #2222 #1111 #1234\n",
    "rd.seed(seed_value) # set random seed\n",
    "np.random.seed(seed_value) # set numpy seed\n",
    "torch.manual_seed(seed_value) # set pytorch seed CPU\n",
    "torch.cuda.manual_seed(seed_value) # set pytorch seed GPU\n",
    "\n",
    "# init training parameters\n",
    "num_epochs = 50\n",
    "mini_batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# convert to pytorch tensor - none cuda enabled\n",
    "dataloader = DataLoader(torch_dataset, batch_size=mini_batch_size, shuffle=True, num_workers=0)\n",
    "# set num_workers to zero to retreive deterministic results\n",
    "\n",
    "# determine if CUDA is available at compute node\n",
    "if (torch.backends.cudnn.version() != None):\n",
    "\n",
    "    dataloader = DataLoader(torch_dataset.cuda(), batch_size=mini_batch_size, shuffle=True)\n",
    "\n",
    "# define optimization criterion and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "encoder_optimizer = torch.optim.Adam(encoder_train.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.Adam(decoder_train.parameters(), lr=learning_rate)\n",
    "\n",
    "# train autoencoder model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # init mini batch counter\n",
    "    mini_batch_count = 0\n",
    "\n",
    "    # determine if CUDA is available at compute node\n",
    "    if (torch.backends.cudnn.version() != None) and (use_cuda == True):\n",
    "\n",
    "        # set all networks / models in CPU mode\n",
    "        encoder_train.cuda()\n",
    "        decoder_train.cuda()\n",
    "\n",
    "    # set networks in training mode (apply dropout when needed)\n",
    "    encoder_train.train()\n",
    "    decoder_train.train()\n",
    "\n",
    "    for mini_batch_data in dataloader:\n",
    "\n",
    "        # increase mini batch counter\n",
    "        mini_batch_count += 1\n",
    "\n",
    "        # convert mini batch to torch variable\n",
    "        mini_batch_torch = autograd.Variable(mini_batch_data)\n",
    "\n",
    "        # =================== forward pass =====================\n",
    "\n",
    "        # run forward pass\n",
    "        z_representation = encoder_train(mini_batch_torch) # encode mini-batch data\n",
    "        mini_batch_reconstruction = decoder_train(z_representation) # decode mini-batch data\n",
    "\n",
    "        # determine reconstruction loss\n",
    "        reconstruction_loss = criterion(mini_batch_reconstruction, mini_batch_torch)\n",
    "\n",
    "        # =================== backward pass ====================\n",
    "\n",
    "        # reset graph gradients\n",
    "        decoder_optimizer.zero_grad()\n",
    "        encoder_optimizer.zero_grad()\n",
    "\n",
    "        # run backward pass\n",
    "        reconstruction_loss.backward()\n",
    "\n",
    "        # update network parameters\n",
    "        decoder_optimizer.step()\n",
    "        encoder_optimizer.step()\n",
    "\n",
    "        # =================== log ==============================\n",
    "\n",
    "        if mini_batch_count % 100 == 0:\n",
    "\n",
    "            # print mini batch reconstuction results\n",
    "            now = datetime.utcnow().strftime(\"%Y%m%d-%H_%M_%S\")\n",
    "            print('[LOG {}] training status, epoch: [{:04}/{:04}], batch: {:04}, loss: {:.10f}'.format(now, (epoch+1), num_epochs, mini_batch_count, reconstruction_loss.data[0]))\n",
    "\n",
    "    # save trained encoder model file to disk\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d-%H_%M_%S\")\n",
    "    encoder_model_name = \"{}_ep_{}_encoder_model.pth\".format(now, (epoch+1))\n",
    "    torch.save(encoder_train.state_dict(), os.path.join(\"models\", encoder_model_name))\n",
    "\n",
    "    # save trained decoder model file to disk\n",
    "    decoder_model_name = \"{}_ep_{}_decoder_model.pth\".format(now, (epoch+1))\n",
    "    torch.save(decoder_train.state_dict(), os.path.join(\"models\", decoder_model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06. Result Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# specify trained models to be loaded\n",
    "#encoder_model_name = \"20180125-06_44_14_ep_1_encoder_model.pth\"\n",
    "#decoder_model_name = \"20180125-06_44_14_ep_1_decoder_model.pth\"\n",
    "\n",
    "# init training network classes / architectures\n",
    "encoder_eval = encoder()\n",
    "decoder_eval = decoder()\n",
    "\n",
    "# load trained models\n",
    "encoder_eval.load_state_dict(torch.load(os.path.join(\"models\", encoder_model_name)))\n",
    "decoder_eval.load_state_dict(torch.load(os.path.join(\"models\", decoder_model_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert encoded transactional data to torch Variable\n",
    "data = autograd.Variable(torch_dataset)\n",
    "\n",
    "# set networks in training mode (don't apply dropout)\n",
    "encoder_eval.eval()\n",
    "decoder_eval.eval()\n",
    "\n",
    "# reconstruct encoded transactional data\n",
    "reconstruction = decoder_eval(encoder_eval(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# determine reconstruction loss - all transactions\n",
    "reconstruction_loss_all = criterion(reconstruction, data)\n",
    "\n",
    "# print reconstruction loss - all transactions\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] collected reconstruction loss of: {:06}/{:06} transactions'.format(now, reconstruction.size()[0], reconstruction.size()[0]))\n",
    "print('[LOG {}] reconstruction loss: {:.10f}'.format(now, reconstruction_loss_all.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# init binary cross entropy errors\n",
    "reconstruction_loss_transaction = np.zeros(reconstruction.size()[0])\n",
    "\n",
    "# iterate over all detailed reconstructions\n",
    "for i in range(0, reconstruction.size()[0]):\n",
    "\n",
    "    # determine reconstruction loss - individual transactions\n",
    "    reconstruction_loss_transaction[i] = criterion(reconstruction[i], data[i])\n",
    "\n",
    "    if(i % 100000 == 0):\n",
    "\n",
    "        ### print conversion summary\n",
    "        now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "        print('[LOG {}] collected individial reconstruction loss of: {:06}/{:06} transactions'.format(now, i, reconstruction.size()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plot_data = np.column_stack((np.array(ori_dataset.ID_Key), np.array(ori_dataset.Artifical_Outlier), reconstruction_loss_transaction))\n",
    "np.random.shuffle(plot_data)\n",
    "\n",
    "order = np.array(range(0, len(ids)))\n",
    "plot_data = np.column_stack((plot_data, order))\n",
    "\n",
    "regular_x = plot_data[np.where(plot_data[:,1]==0)][:,3]\n",
    "regular_y = plot_data[np.where(plot_data[:,1]==0)][:,2]\n",
    "\n",
    "# plot error scatter plot\n",
    "ax.scatter(regular_x, regular_y, c='C0', alpha=0.4, marker=\"o\") # plot regular transactions\n",
    "\n",
    "\n",
    "anomaly_1_x = plot_data[np.where(plot_data[:,1]==1)][:,3]\n",
    "anomaly_1_y = plot_data[np.where(plot_data[:,1]==1)][:,2]\n",
    "\n",
    "# plot error scatter plot\n",
    "ax.scatter(anomaly_1_x, anomaly_1_y, c='C1', marker=\"^\") # plot regular transactions\n",
    "\n",
    "\n",
    "anomaly_2_x = plot_data[np.where(plot_data[:,1]==2)][:,3]\n",
    "anomaly_2_y = plot_data[np.where(plot_data[:,1]==2)][:,2]\n",
    "\n",
    "# plot error scatter plot\n",
    "ax.scatter(anomaly_2_x, anomaly_2_y, c='C3', marker=\"^\") # plot regular transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07. Optional Excercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo - Timur and Marco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08. Lab Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo - Timur and Marco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 09. Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo - Timur and Marco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
